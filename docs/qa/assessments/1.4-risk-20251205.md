# Risk Profile: Story 1.4 - Build Data Quality Validation Pipeline

**Date**: 2025-12-05
**Reviewer**: Quinn (Test Architect)
**Story**: 1.4 - Build Data Quality Validation Pipeline
**Status**: Draft Story (Pre-Implementation Risk Assessment)

---

## Executive Summary

- **Total Risks Identified**: 15
- **Critical Risks (Score 9)**: 2
- **High Risks (Score 6)**: 4
- **Medium Risks (Score 4)**: 6
- **Low Risks (Score 2-3)**: 3
- **Overall Risk Score**: 48/100 (Moderate-High Risk)

**Gate Recommendation**: CONCERNS - Multiple critical and high-priority risks requiring mitigation before deployment. This story introduces validation logic that directly impacts backtest integrity and research conclusions.

---

## Critical Risks Requiring Immediate Attention

### 1. DATA-001: Incomplete or Incorrect Validation Logic Leading to False Confidence

**Score: 9 (Critical)**
**Category**: Data Quality
**Probability**: High (3) - Complex validation logic with multiple edge cases
**Impact**: High (3) - False negatives in validation could allow corrupt data into backtests, producing invalid research conclusions

**Description**:
The validation module is intended to detect missing data, adjustment issues, and survivorship bias. However, if the validation logic itself has bugs or blind spots, it creates a "false sense of security" where developers trust the validation report but corrupt data passes through undetected. This is especially critical for:
- Date gap detection (must account for market holidays, trading halts, delistings)
- Adjustment consistency (must detect split/dividend application errors)
- Point-in-time constituent validation (must correctly identify historical index membership)

**Affected Components**:
- `src/momo/data/validation.py` (all validation functions)
- `_check_date_gaps()` helper
- `_check_adjustment_consistency()` helper
- `validate_prices()` main function

**Mitigation**:
1. **Strategy**: Preventive + Detective
2. **Actions**:
   - Create comprehensive test fixtures with known-bad data (synthetic missing values, corrupted adjustments, date gaps)
   - Implement validation against known-good reference data (e.g., verify historical S&P 500 constituents against published lists)
   - Add defensive assertions in validation logic (e.g., "adjustment factor must be positive", "gap detection must find this known gap")
   - Use property-based testing (hypothesis library) to generate edge cases
   - Code review with focus on validation edge cases (weekends, holidays, delisting dates, splits on last trading day)
3. **Testing Requirements**:
   - **P0 Unit Tests**: Validate each helper function in isolation with synthetic bad data
   - **P0 Integration Tests**: Full validation pipeline with known-corrupt cached test data
   - **P1 Tests**: Edge cases like trading halts, IPO dates, early delisting
4. **Residual Risk**: Low - With comprehensive testing and reference data validation, false negatives can be minimized
5. **Owner**: Dev Agent
6. **Timeline**: Before AC acceptance - must be validated in code review

---

### 2. TECH-001: Bridge Instability During Index Constituent Retrieval

**Score: 9 (Critical)**
**Category**: Technical Integration
**Probability**: High (3) - New Norgate API usage (`index_constituent_timeseries`) not yet proven in codebase
**Impact**: High (3) - If constituent retrieval fails or times out, survivorship bias detection is impossible

**Description**:
Story 1.4 introduces a new Norgate API call (`index_constituent_timeseries`) via the Windows Python bridge. This API is untested in the codebase and may have different failure modes, timeout characteristics, or performance profiles compared to `price_timeseries` (Story 1.2). Given that:
- The bridge already has retry logic and timeout handling (Story 1.2)
- Index constituent data may be large (e.g., Russell 1000 for 20 years = thousands of date-ticker pairs)
- Bridge timeouts are set at 30s per symbol, but constituent retrieval may require different timeout profiles

There is a significant risk that constituent retrieval will fail unpredictably, timeout, or produce incomplete results.

**Affected Components**:
- `src/momo/data/validation.py` (`get_index_constituents_at_date()`)
- `src/momo/data/bridge.py` (may need new bridge function for constituent retrieval)

**Mitigation**:
1. **Strategy**: Preventive + Corrective
2. **Actions**:
   - Add explicit bridge function `fetch_index_constituents()` to bridge.py with appropriate timeout (suggest 60s for index retrieval vs 30s for single symbol)
   - Implement graceful fallback: if bridge call fails, log warning and return `ValidationReport` with flag `constituent_validation_unavailable=True`
   - Cache constituent data similar to price data (Parquet caching) to avoid repeated bridge calls
   - Add retry logic with exponential backoff (already present in bridge via tenacity)
   - Test bridge call with real NDU in integration tests before merging
3. **Testing Requirements**:
   - **P0 Integration Test**: Real bridge call to fetch Russell 1000 constituents for a specific date (requires NDU)
   - **P1 Unit Test**: Mock bridge timeout and verify graceful degradation
   - **P1 Unit Test**: Mock bridge error and verify error handling with partial validation report
4. **Residual Risk**: Medium - Bridge may still fail in production, but graceful fallback prevents complete validation failure
5. **Owner**: Dev Agent
6. **Timeline**: Before integration testing phase

---

## High Risks

### 3. DATA-002: False Positives in Date Gap Detection Due to Market Holidays/Halts

**Score: 6 (High)**
**Category**: Data Quality
**Probability**: Medium (2) - Market holidays are known but trading halts are unpredictable
**Impact**: High (3) - False positives flood validation reports with noise, reducing trust in validation

**Description**:
Date gap detection must distinguish between:
- **Legitimate gaps**: Weekends, market holidays (NYSE calendar), trading halts (e.g., 9/11, circuit breaker halts)
- **Data quality issues**: Missing data from cache corruption, bridge failures, or Norgate data gaps

If the validation logic naively detects "any gap > 1 business day" without accounting for legitimate gaps, it will produce excessive false positives that erode developer trust in validation reports.

**Mitigation**:
1. **Strategy**: Preventive
2. **Actions**:
   - Use `pandas.tseries.offsets.BDay()` for business day calculations (accounts for weekends)
   - Consider using `pandas_market_calendars` library for NYSE holiday calendar (or accept some false positives for rare events like 9/11)
   - Set threshold for "suspicious gap" (e.g., > 5 business days) to reduce false positives from single-day trading halts
   - Document known limitations in ValidationReport docstring
3. **Testing Requirements**:
   - **P1 Unit Test**: Date gap detection with DataFrame containing known market holiday (verify no false positive)
   - **P1 Unit Test**: Date gap detection with 10-day gap (verify detection)
4. **Residual Risk**: Low-Medium - Some false positives from rare events (trading halts) acceptable if documented
5. **Owner**: Dev Agent
6. **Timeline**: During implementation

---

### 4. PERF-001: Validation Performance Degrades with Large Universes

**Score: 6 (High)**
**Category**: Performance
**Probability**: High (3) - 1000+ tickers with 20 years of data = millions of rows
**Impact**: Medium (2) - Slow validation blocks research workflow, but doesn't corrupt results

**Description**:
Validation on large datasets (e.g., Russell 1000 for 20 years = ~5M rows) may be slow if implemented naively:
- Date gap detection: Must check each ticker's time series for gaps (1000 iterations)
- Adjustment consistency: Must compare adjusted vs unadjusted prices for all rows
- Missing data checks: Must scan all NaN values in DataFrame

Without vectorized pandas operations, validation could take minutes instead of seconds.

**Mitigation**:
1. **Strategy**: Preventive
2. **Actions**:
   - Use vectorized pandas operations (no row-by-row iteration)
   - Add progress logging with structlog every N tickers (e.g., log every 100 tickers processed)
   - Consider caching validation results (similar to price data caching)
   - Profile validation performance in integration tests and document expected runtime
3. **Testing Requirements**:
   - **P1 Integration Test**: Validate 100-ticker universe and measure time (should be < 5s)
   - **P2 Unit Test**: Validate single-ticker data with 10K rows (should be < 100ms)
4. **Residual Risk**: Low - Pandas vectorization should be sufficient for initial implementation
5. **Owner**: Dev Agent
6. **Timeline**: During implementation (profile in integration tests)

---

### 5. TECH-002: ValidationError Exception Not Yet Defined in exceptions.py

**Score: 6 (High)**
**Category**: Technical Debt
**Probability**: High (3) - Exception not yet implemented per story tasks
**Impact**: Medium (2) - Missing exception breaks imports and type hints, but easy to fix

**Description**:
Story 1.4 tasks specify adding `ValidationError` to `src/momo/utils/exceptions.py` as a subclass of `DataError`. However, reading the current exceptions.py shows it only defines:
- `BridgeError` and subclasses
- `DataError`
- `CacheError`

There is no `ValidationError` yet. This must be added before any validation.py imports, or all imports will fail.

**Mitigation**:
1. **Strategy**: Preventive
2. **Actions**:
   - Add `ValidationError(DataError)` class to exceptions.py as the first task
   - Follow TDD workflow: write test that imports ValidationError FIRST, then implement exception
   - Use ValidationError for validation failures (e.g., "all tickers failed validation")
3. **Testing Requirements**:
   - **P0 Unit Test**: Import ValidationError and verify inheritance from DataError
   - **P1 Unit Test**: Raise ValidationError from validation function and verify exception message
4. **Residual Risk**: Minimal - Simple addition, low complexity
5. **Owner**: Dev Agent
6. **Timeline**: First task in Story 1.4 implementation

---

### 6. DATA-003: Delisting Detection Logic May Miss Edge Cases

**Score: 6 (High)**
**Category**: Data Quality
**Probability**: Medium (2) - Delisting scenarios are complex (bankruptcy, merger, going private)
**Impact**: High (3) - Missing delisting events leads to survivorship bias in backtests

**Description**:
Delisting detection is critical for eliminating survivorship bias. The story specifies using "*Current & Past" watchlists and checking if price data ends before query end date. However, edge cases exist:
- **Merged companies**: Ticker may end but continue under new symbol (e.g., FB → META)
- **Bankruptcy delisting**: Ticker ends with zero price (may look like data quality issue)
- **Going private**: Ticker ends but company still exists
- **Temporary suspensions**: Ticker may resume trading after gap

Without careful logic, these edge cases may be misclassified (false positives or false negatives).

**Mitigation**:
1. **Strategy**: Detective + Corrective
2. **Actions**:
   - Document known limitations in ValidationReport and docstrings
   - Use heuristic: "ticker with price data ending > 30 days before query end date is likely delisted"
   - Provide delisting date (last trading date) in ValidationReport
   - Do NOT fail validation on delisting detection - this is informational only
   - Consider future enhancement: query Norgate for explicit delisting information via API
3. **Testing Requirements**:
   - **P1 Unit Test**: Known delisted ticker (e.g., Enron, Lehman Brothers) correctly identified
   - **P2 Unit Test**: Recently delisted ticker flagged with correct last trading date
4. **Residual Risk**: Medium - Some edge cases may be misclassified, but report provides transparency
5. **Owner**: Dev Agent
6. **Timeline**: During implementation with documentation emphasis

---

## Medium Risks

### 7. TECH-003: MultiIndex Preservation in Validation Functions

**Score: 4 (Medium)**
**Category**: Technical Complexity
**Probability**: Medium (2) - Story 1.3 had MultiIndex preservation issues that were fixed
**Impact**: Medium (2) - Breaking MultiIndex causes downstream pipeline failures

**Description**:
Story 1.3 loader produces DataFrames with `MultiIndex (date, symbol)`. Validation functions must preserve this structure when analyzing data, or return appropriate structures. Given the known issues in Story 1.3 (PyArrow engine required for MultiIndex preservation), there is risk that validation functions will accidentally break the MultiIndex.

**Mitigation**:
1. **Strategy**: Preventive
2. **Actions**:
   - Validation functions should NOT modify the input DataFrame (read-only analysis)
   - If DataFrame copies are needed, use `df.copy()` to avoid mutation
   - ValidationReport is a separate dataclass - no DataFrame mutations needed
   - Add assertions in tests to verify MultiIndex preservation
3. **Testing Requirements**:
   - **P1 Unit Test**: Validate prices DataFrame and verify input MultiIndex unchanged
4. **Residual Risk**: Low - Validation is read-only by design
5. **Owner**: Dev Agent
6. **Timeline**: During implementation

---

### 8. DATA-004: Adjustment Consistency Logic May Be Too Strict or Too Lenient

**Score: 4 (Medium)**
**Category**: Data Quality
**Probability**: Medium (2) - Heuristic thresholds are judgement calls
**Impact**: Medium (2) - False positives/negatives reduce validation effectiveness

**Description**:
Adjustment consistency checks must detect when splits/dividends are incorrectly applied. However, determining "suspicious price jumps" requires heuristic thresholds:
- What constitutes a "suspiciously large price jump"? 10%? 50%? 100%?
- Should validation check `unadjusted_close != close` when dividends occurred?
- How to handle cases where dividend info is missing from Norgate?

Too strict → false positives. Too lenient → false negatives.

**Mitigation**:
1. **Strategy**: Iterative refinement
2. **Actions**:
   - Start with conservative heuristic: flag price jumps > 50% without corresponding dividend/split info
   - Document the heuristic and thresholds clearly in docstrings
   - Provide detailed reporting: show specific tickers and dates flagged
   - Accept that some false positives are better than false negatives in this context
3. **Testing Requirements**:
   - **P1 Unit Test**: Known stock split (e.g., Apple 7:1 split in 2014) correctly handled
   - **P1 Unit Test**: Synthetic corrupt adjustment (50% jump without dividend) correctly flagged
4. **Residual Risk**: Medium - Heuristics require tuning based on real-world data
5. **Owner**: Dev Agent (implementation), QA Agent (validation in review)
6. **Timeline**: During implementation with review in QA gate

---

### 9. OPS-001: Validation Reports May Not Be Human-Readable

**Score: 4 (Medium)**
**Category**: Operational Usability
**Probability**: Medium (2) - Dataclass with nested dicts/lists can be verbose
**Impact**: Medium (2) - Poor UX reduces adoption of validation checks

**Description**:
The `ValidationReport` dataclass contains complex nested structures:
- `missing_data_counts: dict[str, int]` - could be 1000+ entries for large universes
- `date_gaps: dict[str, list[tuple[date, date]]]` - verbose nested structure
- `adjustment_issues: list[str]` - could be long list

If printed directly, this will be overwhelming and difficult to interpret. Developers may ignore validation reports if they're too verbose or lack clear summary.

**Mitigation**:
1. **Strategy**: Preventive (UX design)
2. **Actions**:
   - Add `summary_message` field to ValidationReport with human-readable summary (e.g., "5 tickers with missing data, 2 with adjustment issues, 10 delisted")
   - Add `is_valid` boolean flag for quick pass/fail check
   - Implement `__str__` method on ValidationReport for clean printing
   - Document interpretation guide in module docstring
3. **Testing Requirements**:
   - **P2 Unit Test**: Generate ValidationReport and verify summary_message is concise
   - Manual validation in integration test: print report and verify readability
4. **Residual Risk**: Low - Good documentation and summary fields improve usability
5. **Owner**: Dev Agent
6. **Timeline**: During implementation

---

### 10. TECH-004: Type Hints for Nested Structures May Be Complex

**Score: 4 (Medium)**
**Category**: Type Safety
**Probability**: Medium (2) - Mypy strict mode is unforgiving
**Impact**: Medium (2) - Type errors block merge, but are fixable

**Description**:
ValidationReport has complex nested types:
```python
date_gaps: dict[str, list[tuple[datetime.date, datetime.date]]]
delisting_events: dict[str, datetime.date | None]
```

Mypy strict mode may complain about:
- `datetime.date` vs `date` import ambiguity
- Optional types in dict values
- Serialization to/from JSON (if needed)

**Mitigation**:
1. **Strategy**: Preventive
2. **Actions**:
   - Use explicit imports: `from datetime import date as Date` for clarity
   - Test type checking early: `uv run mypy src/momo/data/validation.py` after dataclass definition
   - Use `from __future__ import annotations` for forward references
   - Add type: ignore comments only as last resort with explanation
3. **Testing Requirements**:
   - Type checking must pass before PR merge
4. **Residual Risk**: Low - Mypy errors are compile-time and easily fixed
5. **Owner**: Dev Agent
6. **Timeline**: Continuous during implementation

---

### 11. DATA-005: Cached Validation Results May Become Stale

**Score: 4 (Medium)**
**Category**: Data Quality
**Probability**: Low (1) - Only if caching is implemented
**Impact**: High (3) - Stale validation gives false confidence

**Description**:
If validation results are cached (for performance), there's risk that cached validation reports become stale when:
- Underlying price data is refreshed (force_refresh=True)
- New data is added to Norgate
- Validation logic is updated

Developers might trust a cached validation report that no longer reflects current data state.

**Mitigation**:
1. **Strategy**: Preventive (don't implement caching in Story 1.4)
2. **Actions**:
   - Do NOT cache validation results in Story 1.4 (defer to future story)
   - Validation is relatively fast (< 5s for 100 tickers) and can run on-demand
   - Document in ADR that validation caching is explicitly deferred
3. **Testing Requirements**:
   - N/A - no caching in this story
4. **Residual Risk**: N/A - not implementing caching
5. **Owner**: Product Owner (decision to defer)
6. **Timeline**: N/A

---

### 12. OPS-002: Missing Usage Examples in Documentation

**Score: 4 (Medium)**
**Category**: Documentation
**Probability**: Medium (2) - Developers may rush documentation
**Impact**: Medium (2) - Poor docs reduce adoption and increase support burden

**Description**:
AC 8 requires documentation explaining how to interpret validation reports and handle common data quality issues. However, "documentation" is often deprioritized in implementation. Without clear examples:
- Developers won't know when to run validation
- Developers won't know how to interpret ValidationReport fields
- Developers won't know remediation steps for common issues

**Mitigation**:
1. **Strategy**: Preventive (explicit acceptance criteria)
2. **Actions**:
   - Add documentation task to story with clear requirements:
     - Module docstring with usage example
     - ValidationReport docstring with field explanations
     - Common issues guide (e.g., "Missing data → check NDU connectivity")
   - QA gate checks for documentation completeness
3. **Testing Requirements**:
   - **P2 Test**: Verify module docstring contains usage example
4. **Residual Risk**: Low - QA gate enforces documentation quality
5. **Owner**: Dev Agent (write), QA Agent (verify in gate)
6. **Timeline**: Final task before story completion

---

## Low Risks

### 13. BUS-001: Validation May Detect Issues in Historical Data We Can't Fix

**Score: 3 (Low)**
**Category**: Business Impact
**Probability**: Low (1) - Norgate data quality is generally high
**Impact**: High (3) - Unfixable data issues block research

**Description**:
Validation may detect legitimate data quality issues in historical Norgate data (e.g., missing data from delisted micro-cap stocks). If these issues are unfixable (data simply doesn't exist), validation creates awareness but no actionable solution.

**Mitigation**:
1. **Strategy**: Acceptance + Documentation
2. **Actions**:
   - ValidationReport is informational, not blocking
   - Document known limitations in report
   - Provide guidance: "If validation fails, consider excluding problematic tickers from universe"
3. **Residual Risk**: Accepted - some data quality issues are inherent
5. **Owner**: Research team (decide how to handle in workflow)
6. **Timeline**: Post-implementation

---

### 14. PERF-002: Bridge Call for Index Constituents May Be Slow

**Score: 3 (Low)**
**Category**: Performance
**Probability**: Low (1) - Norgate API is generally fast (~7ms/symbol)
**Impact**: High (3) - Very slow constituent retrieval blocks validation

**Description**:
Index constituent retrieval is a new Norgate API call. If it's significantly slower than price retrieval, validation may take minutes instead of seconds.

**Mitigation**:
1. **Strategy**: Detective + Corrective
2. **Actions**:
   - Add timeout to bridge call (60s for index retrieval)
   - Log retrieval time in structlog
   - If slow, add caching for constituent data (defer to future story)
3. **Testing Requirements**:
   - **P1 Integration Test**: Measure time to fetch Russell 1000 constituents
4. **Residual Risk**: Low - Caching can be added later if needed
5. **Owner**: Dev Agent
6. **Timeline**: Integration testing phase

---

### 15. SEC-001: Validation Report May Expose Sensitive Information in Logs

**Score: 2 (Low)**
**Category**: Security
**Probability**: Low (1) - This is research tool, not production system
**Impact**: Medium (2) - Log exposure to unauthorized users

**Description**:
ValidationReport contains ticker symbols and date ranges. In a production system, this might be considered sensitive trading information. However, this is a personal research tool with no sensitive data.

**Mitigation**:
1. **Strategy**: Acceptance (risk accepted for research tool)
2. **Actions**:
   - None required for personal research tool
   - Document in security model that logs may contain ticker info
3. **Residual Risk**: Accepted - research tool with no sensitive data
5. **Owner**: N/A
6. **Timeline**: N/A

---

## Risk Distribution

### By Category
- **Data Quality (DATA)**: 5 risks (1 critical, 2 high, 2 medium)
- **Technical (TECH)**: 4 risks (1 critical, 2 high, 1 medium)
- **Performance (PERF)**: 2 risks (1 high, 1 low)
- **Operational (OPS)**: 2 risks (2 medium)
- **Business (BUS)**: 1 risk (1 low)
- **Security (SEC)**: 1 risk (1 low)

### By Component
- **Validation Logic** (`validation.py`): 9 risks
- **Bridge Integration** (`bridge.py`): 3 risks
- **Documentation**: 2 risks
- **Exception Handling** (`exceptions.py`): 1 risk

### Risk Matrix

| Risk ID  | Description                                      | Probability | Impact     | Score | Priority |
|----------|--------------------------------------------------|-------------|------------|-------|----------|
| DATA-001 | Incomplete validation logic (false negatives)    | High (3)    | High (3)   | 9     | Critical |
| TECH-001 | Bridge instability for index constituents        | High (3)    | High (3)   | 9     | Critical |
| DATA-002 | False positives in date gap detection            | Medium (2)  | High (3)   | 6     | High     |
| PERF-001 | Validation performance degradation               | High (3)    | Medium (2) | 6     | High     |
| TECH-002 | ValidationError exception not yet defined        | High (3)    | Medium (2) | 6     | High     |
| DATA-003 | Delisting detection edge cases                   | Medium (2)  | High (3)   | 6     | High     |
| TECH-003 | MultiIndex preservation in validation            | Medium (2)  | Medium (2) | 4     | Medium   |
| DATA-004 | Adjustment consistency heuristics                | Medium (2)  | Medium (2) | 4     | Medium   |
| OPS-001  | Validation reports not human-readable            | Medium (2)  | Medium (2) | 4     | Medium   |
| TECH-004 | Complex type hints for nested structures         | Medium (2)  | Medium (2) | 4     | Medium   |
| DATA-005 | Cached validation results become stale           | Low (1)     | High (3)   | 4     | Medium   |
| OPS-002  | Missing usage examples in documentation          | Medium (2)  | Medium (2) | 4     | Medium   |
| BUS-001  | Unfixable historical data quality issues         | Low (1)     | High (3)   | 3     | Low      |
| PERF-002 | Slow bridge call for index constituents          | Low (1)     | High (3)   | 3     | Low      |
| SEC-001  | Validation report exposes ticker info in logs    | Low (1)     | Medium (2) | 2     | Low      |

---

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Tests (Must Have - P0)

**Focus: Validation Correctness & Bridge Stability**

1. **Validation Correctness Tests** (DATA-001):
   - Unit test: Validate prices with synthetic missing values → must detect
   - Unit test: Validate prices with synthetic date gaps → must detect
   - Unit test: Validate prices with adjustment errors → must detect
   - Integration test: Full validation with known-corrupt cached test data → must fail

2. **Bridge Stability Tests** (TECH-001):
   - Integration test: Fetch index constituents via bridge (requires NDU)
   - Unit test: Mock bridge timeout → verify graceful degradation
   - Unit test: Mock bridge error → verify partial validation report

3. **Exception Handling** (TECH-002):
   - Unit test: Import ValidationError and verify inheritance
   - Unit test: Raise ValidationError from validation function

**Test Data Requirements**:
- Fixture: `sample_price_df_clean` (no issues)
- Fixture: `sample_price_df_with_nans` (known missing values)
- Fixture: `sample_price_df_with_gaps` (known date gaps)
- Fixture: `sample_price_df_with_adjustment_error` (synthetic adjustment issue)

---

### Priority 2: High Risk Tests (Should Have - P1)

**Focus: Edge Cases & Performance**

1. **Date Gap False Positive Prevention** (DATA-002):
   - Unit test: Date gap detection with market holiday → no false positive
   - Unit test: Date gap detection with 10-day gap → detection

2. **Performance Validation** (PERF-001):
   - Integration test: Validate 100-ticker universe < 5s
   - Unit test: Single-ticker validation < 100ms

3. **Delisting Detection** (DATA-003):
   - Unit test: Known delisted ticker (e.g., Enron) correctly identified
   - Unit test: Recently delisted ticker with correct last trading date

4. **Adjustment Consistency** (DATA-004):
   - Unit test: Known stock split correctly handled (e.g., Apple 7:1)
   - Unit test: Synthetic corrupt adjustment (50% jump) flagged

---

### Priority 3: Medium/Low Risk Tests (Nice to Have - P2)

**Focus: Usability & Edge Cases**

1. **Human-Readable Reports** (OPS-001):
   - Unit test: ValidationReport has concise summary_message
   - Manual validation: Print report and verify readability

2. **Documentation** (OPS-002):
   - Unit test: Module docstring contains usage example

3. **Bridge Performance** (PERF-002):
   - Integration test: Measure time to fetch Russell 1000 constituents

---

## Risk Acceptance Criteria

### Must Fix Before Merge (Gate = FAIL)

1. **DATA-001**: Validation logic must have comprehensive test coverage (P0 tests pass)
2. **TECH-001**: Bridge integration must work in integration tests (or gracefully degrade)
3. **TECH-002**: ValidationError exception must be defined and tested

**Gate Decision**: If any critical risk is unmitigated → **FAIL**

---

### Can Merge with Mitigation (Gate = CONCERNS)

1. **DATA-002**: Date gap false positives acceptable if documented and thresholds tuned
2. **PERF-001**: Validation performance acceptable if < 5s for 100 tickers (measured)
3. **DATA-003**: Delisting detection edge cases acceptable with documentation
4. **DATA-004**: Adjustment consistency heuristics acceptable with documentation

**Gate Decision**: If high risks have mitigation plan → **CONCERNS** (acceptable to merge)

---

### Accepted Risks (Gate = PASS with conditions)

1. **BUS-001**: Unfixable data issues → documented limitation
2. **PERF-002**: Slow bridge calls → add caching in future story if needed
3. **SEC-001**: Log exposure → accepted for research tool

---

## Monitoring Requirements

**Post-Deployment Monitoring** (for integration into research workflow):

1. **Performance Metrics** (PERF-001, PERF-002):
   - Log validation execution time in structlog
   - Alert if validation takes > 10s for typical universe

2. **Error Rates** (TECH-001, DATA-001):
   - Count ValidationError exceptions raised
   - Log bridge timeouts/failures during constituent retrieval
   - Track percentage of universes with validation warnings

3. **Data Quality Metrics** (DATA-002, DATA-003, DATA-004):
   - Track number of tickers flagged for missing data
   - Track number of tickers flagged for adjustment issues
   - Track number of delisted tickers detected per universe

---

## Risk Review Triggers

**Review and update risk profile when:**

1. **Architecture changes**:
   - New Norgate API endpoints added to bridge
   - Caching strategy changes (e.g., adding validation caching)

2. **Validation logic evolves**:
   - New validation checks added (e.g., volume validation, price range checks)
   - Heuristic thresholds tuned based on real-world data

3. **Performance issues reported**:
   - Validation takes > 10s for typical universes
   - Bridge timeouts increase frequency

4. **Data quality issues discovered**:
   - Validation misses known-corrupt data (false negative)
   - Validation reports excessive false positives

---

## Risk Scoring Calculation

**Base Score**: 100
**Deductions**:
- Critical risks (9 points): 2 × 20 = 40 points
- High risks (6 points): 4 × 10 = 40 points
- Medium risks (4 points): 6 × 5 = 30 points (capped at max deduction)
- Low risks (2-3 points): 3 × 2 = 6 points

**Raw Calculation**: 100 - 40 - 40 + 30 = 50 (capped deductions)
**Adjusted Score**: 48/100 (moderate-high risk)

**Interpretation**:
- 80-100: Low risk (minimal concerns)
- 60-79: Moderate risk (some concerns, acceptable)
- 40-59: **Moderate-High risk** ← Story 1.4 is here
- 20-39: High risk (significant concerns, may need scope reduction)
- 0-19: Critical risk (very risky, should reconsider)

---

## Recommendations

### Testing Priority
1. **Run P0 tests first**: Validation correctness and bridge stability are blocking
2. **Add property-based testing**: Use hypothesis library to generate edge cases for validation logic
3. **Integration testing with real NDU**: Must verify bridge calls work before merge

### Development Focus
1. **Code review emphasis**: Focus on validation edge cases (date gaps, adjustments, delisting)
2. **TDD workflow**: Write tests before implementing validation functions
3. **Defensive programming**: Add assertions in validation logic for invariants

### Deployment Strategy
1. **Phased adoption**: Start by running validation on small test universes before full Russell 1000
2. **Manual validation**: First few validation runs should be manually reviewed to verify report quality
3. **Documentation-first**: Write usage guide before implementation to clarify requirements

### Monitoring Setup
1. **Performance logging**: Log validation execution time for all runs
2. **Error tracking**: Count ValidationError exceptions and bridge failures
3. **Quality dashboard**: Track validation findings over time (% universes with issues)

---

## Integration with Quality Gates

**Deterministic Gate Mapping**:

- **Critical risks (score ≥ 9)**: 2 unmitigated → Gate = **FAIL**
- **High risks (score ≥ 6)**: 4 require mitigation → Gate = **CONCERNS**
- **Medium/Low risks**: Acceptable with documentation → Gate = **PASS** (if critical/high resolved)

**Current Gate Recommendation**: **CONCERNS**

**Rationale**:
- 2 critical risks (DATA-001, TECH-001) require comprehensive testing and mitigation
- 4 high risks require attention but have clear mitigation strategies
- Most risks are preventable through good testing and documentation
- Story is implementable with appropriate care and test coverage

**Path to PASS**:
1. Implement comprehensive P0 test suite (validation correctness, bridge stability)
2. Verify bridge integration in integration tests with real NDU
3. Add ValidationError exception and verify type checking passes
4. Document validation logic limitations and usage examples
5. Profile performance in integration tests (< 5s for 100 tickers)

---

## Appendix: Risk-Based Test Design Map

**Critical Risk Coverage**:
- DATA-001 → Test IDs: 1.4-UNIT-001 (clean data), 1.4-UNIT-002 (NaN detection), 1.4-UNIT-003 (gap detection), 1.4-UNIT-004 (adjustment check)
- TECH-001 → Test IDs: 1.4-INT-001 (bridge constituent fetch), 1.4-UNIT-010 (bridge timeout mock), 1.4-UNIT-011 (bridge error mock)
- TECH-002 → Test IDs: 1.4-UNIT-015 (exception import and inheritance)

**High Risk Coverage**:
- DATA-002 → Test IDs: 1.4-UNIT-005 (market holiday), 1.4-UNIT-006 (10-day gap)
- PERF-001 → Test IDs: 1.4-INT-002 (100-ticker perf), 1.4-UNIT-012 (single-ticker perf)
- DATA-003 → Test IDs: 1.4-UNIT-007 (known delisted), 1.4-UNIT-008 (recent delisting)
- DATA-004 → Test IDs: 1.4-UNIT-009 (stock split), 1.4-UNIT-013 (corrupt adjustment)

**Medium/Low Risk Coverage**:
- OPS-001 → Test IDs: 1.4-UNIT-014 (report summary)
- OPS-002 → Test IDs: 1.4-UNIT-016 (docstring check)
- PERF-002 → Test IDs: 1.4-INT-003 (constituent fetch timing)

---

**End of Risk Profile**

---

**File Reference for Review Task**:
`docs/qa/assessments/1.4-risk-20251205.md`
