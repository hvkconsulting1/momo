# Test Design: Story 1.3 - Implement Data Loading and Parquet Caching

**Date**: 2025-12-04
**Designer**: Quinn (Test Architect)
**Story**: 1.3 - Implement Data Loading and Parquet Caching

## Test Strategy Overview

- **Total test scenarios**: 26
- **Unit tests**: 18 (69%)
- **Integration tests**: 8 (31%)
- **E2E tests**: 0 (0%) - Not applicable for data infrastructure layer
- **Priority distribution**: P0: 16 (62%), P1: 7 (27%), P2: 3 (11%)

### Strategy Rationale

This story implements foundational data infrastructure (cache manager and data loader) without user-facing components, making E2E testing inapplicable. The focus is on:

1. **Unit testing dominance** (69%) - Appropriate for logic-heavy cache management and orchestration
2. **High P0 proportion** (62%) - Reflects data integrity criticality in backtesting framework
3. **Risk-driven coverage** - Prioritizes high-risk areas (schema drift, partial failures) identified in risk profile
4. **Efficient layering** - Unit tests mock I/O; integration tests validate actual filesystem operations

## Test Scenarios by Acceptance Criteria

### AC1: Fetch OHLCV data using norgatedata API

#### Scenarios

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-UNIT-001 | Unit | P0 | Verify `load_universe()` fetches single symbol via bridge when cache misses | Core orchestration logic; validates bridge integration pattern | TECH-002 |
| 1.3-UNIT-002 | Unit | P0 | Verify `load_universe()` creates cache directories if missing using `Path.mkdir(parents=True, exist_ok=True)` | Prevents runtime failures; validates AC1 file system setup | OPS-001 |
| 1.3-UNIT-003 | Unit | P1 | Verify `load_universe()` logs progress for multi-symbol fetches using structlog | Observability for long-running operations; validates AC1 progress tracking | N/A |

**AC1 Coverage**: 3 tests (2 P0, 1 P1) - Validates core fetching orchestration with mocked bridge calls

---

### AC2: Fetched data includes adjustment factors (TOTALRETURN)

#### Scenarios

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-UNIT-004 | Unit | P0 | Verify `load_universe()` passes `adjustment="TOTALRETURN"` to bridge calls | Data integrity critical; ensures dividend-adjusted prices | DATA-001 |
| 1.3-INT-001 | Integration | P0 | Verify fetched data includes dividend column when using TOTALRETURN adjustment | End-to-end validation of adjustment type propagation | DATA-001 |

**AC2 Coverage**: 2 tests (both P0) - Critical for backtest accuracy; validates adjustment type handling

---

### AC3: Cache to Parquet with organized naming

#### Scenarios

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-UNIT-005 | Unit | P0 | Verify `get_cache_path()` generates consistent paths following `{universe}_{start_date}_{end_date}.parquet` pattern | Deterministic caching foundation; prevents path collisions | TECH-001 |
| 1.3-UNIT-006 | Unit | P0 | Verify `save_prices()` validates DataFrame schema before writing (required columns present) | Prevents schema drift; early failure detection | DATA-001 |
| 1.3-UNIT-007 | Unit | P0 | Verify `save_prices()` validates DataFrame dtypes match expected schema | Data integrity; prevents silent type coercion errors | DATA-001 |
| 1.3-UNIT-008 | Unit | P0 | Verify `save_prices()` validates MultiIndex structure `(date, symbol)` exists | Critical for downstream processing; prevents index corruption | DATA-002 |
| 1.3-UNIT-009 | Unit | P0 | Verify `save_prices()` rejects empty DataFrames with informative exception | Prevents masking fetch failures; early error detection | DATA-004 |
| 1.3-UNIT-010 | Unit | P1 | Verify `save_prices()` includes metadata (universe, date range, created_at, schema_version) in Parquet file | Enables staleness detection and cache versioning | DATA-003, DATA-001 |
| 1.3-INT-002 | Integration | P0 | Verify `save_prices()` writes Parquet to correct path in `data/cache/prices/` directory | Validates AC3 file organization | N/A |
| 1.3-INT-003 | Integration | P1 | Verify Parquet files use snappy compression and pyarrow engine as specified | Performance optimization; validates tech stack alignment | PERF-002 |

**AC3 Coverage**: 8 tests (6 P0, 2 P1) - High priority due to schema drift risk (DATA-001)

---

### AC4: Cache loading checks local Parquet before API query

#### Scenarios

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-UNIT-011 | Unit | P0 | Verify `load_prices()` returns None when cache file does not exist | Core cache miss logic; enables orchestrator fallback | N/A |
| 1.3-UNIT-012 | Unit | P0 | Verify `load_prices()` returns DataFrame when cache file exists and is valid | Core cache hit logic; validates primary caching benefit | N/A |
| 1.3-UNIT-013 | Unit | P0 | Verify `load_universe()` calls bridge only when `load_prices()` returns None (cache miss) | Orchestration correctness; prevents unnecessary API calls | N/A |
| 1.3-UNIT-014 | Unit | P0 | Verify `load_universe()` returns cached data directly when `load_prices()` succeeds (cache hit) | Validates AC4 cache-first strategy | N/A |

**AC4 Coverage**: 4 tests (all P0) - Critical path for caching benefit realization

---

### AC5: Force-refresh option bypasses cache

#### Scenarios

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-UNIT-015 | Unit | P0 | Verify `load_universe(force_refresh=True)` calls bridge even when cache exists | Validates AC5 force-refresh parameter | DATA-003 |
| 1.3-UNIT-016 | Unit | P1 | Verify `invalidate()` function removes cache files for given universe | Cache management utility; supports staleness mitigation | DATA-003 |
| 1.3-INT-004 | Integration | P1 | Verify `force_refresh=True` fetches fresh data and overwrites existing cache | End-to-end validation of refresh workflow | DATA-003 |

**AC5 Coverage**: 3 tests (1 P0, 2 P1) - Supports staleness detection risk mitigation

---

### AC6: Cache >10x faster than API (cache <50ms, API ~500ms for 10 symbols)

#### Scenarios

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-INT-005 | Integration | P1 | Verify cache load time <50ms for 10-symbol dataset | Performance SLA validation; baseline for future optimization | PERF-001 |
| 1.3-INT-006 | Integration | P2 | Verify API fetch time ~500ms for 10 symbols (establishes baseline) | Documents current performance; justifies caching benefit | PERF-001 |
| 1.3-INT-007 | Integration | P1 | Verify cache speedup >10x compared to API fetch for same dataset | Validates AC6 performance requirement | PERF-001 |

**AC6 Coverage**: 3 tests (2 P1, 1 P2) - Performance validation; not blocking but important

---

### AC7: Cached Parquet readable with correct dtypes and index

#### Scenarios

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-UNIT-017 | Unit | P0 | Verify round-trip equality: save → load → DataFrame comparison (schema, values, index) | Comprehensive cache correctness validation | DATA-001, DATA-002 |
| 1.3-INT-008 | Integration | P0 | Verify loaded DataFrame has correct dtypes (float64, int64, datetime64[ns]) after Parquet round-trip | Actual filesystem validation; prevents dtype loss in serialization | DATA-001 |
| 1.3-INT-009 | Integration | P0 | Verify loaded DataFrame preserves MultiIndex `(date, symbol)` structure after Parquet round-trip | Critical for downstream processing; validates index preservation | DATA-002 |

**AC7 Coverage**: 3 tests (all P0) - Data integrity critical for backtesting correctness

---

### AC8: Unit tests verify caching logic

**Note**: AC8 is meta-requirement satisfied by all unit tests above. Additional coverage:

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-UNIT-018 | Unit | P0 | Verify `load_universe()` handles partial fetch failures gracefully (continue with remaining symbols, log errors, save partial results) | Complex error handling; high probability in production (delisted stocks, API errors) | TECH-002 |

**AC8 Coverage**: 1 additional test (P0) - Addresses highest-probability risk (TECH-002)

---

## Risk Coverage Matrix

This section maps test scenarios to risks identified in `1.3-risk-20251204.md`:

### High Risks (Score 6)

#### DATA-001: Parquet Schema Drift
**Mitigation Tests**:
- 1.3-UNIT-004 (P0): Validates TOTALRETURN adjustment parameter
- 1.3-UNIT-006 (P0): Schema validation - required columns present
- 1.3-UNIT-007 (P0): Schema validation - correct dtypes
- 1.3-UNIT-008 (P0): Schema validation - MultiIndex structure
- 1.3-UNIT-010 (P1): Metadata includes schema_version for compatibility detection
- 1.3-UNIT-017 (P0): Round-trip equality ensures schema preservation
- 1.3-INT-001 (P0): Integration validation of adjustment type propagation
- 1.3-INT-008 (P0): Dtype preservation after actual Parquet I/O
- 1.3-INT-009 (P0): MultiIndex preservation after actual Parquet I/O

**Coverage**: 9 tests (7 P0, 2 P1) - Comprehensive mitigation of highest data integrity risk

#### TECH-002: Bridge Integration Error Handling
**Mitigation Tests**:
- 1.3-UNIT-001 (P0): Basic bridge integration pattern validation
- 1.3-UNIT-018 (P0): Partial failure handling with error aggregation and partial cache storage

**Coverage**: 2 tests (both P0) - Addresses high-probability failure scenario

**Gap Identified**: Risk profile recommends 5 additional tests (UNIT-011 through UNIT-013, INT-006, INT-007) for comprehensive partial failure coverage. These are incorporated into the design but require integration test implementation:

| ID | Level | Priority | Test | Justification | Risk Coverage |
|----|-------|----------|------|---------------|---------------|
| 1.3-INT-010 | Integration | P0 | Verify cache consistency when mix of valid and invalid symbols fetched (partial failure scenario) | Real-world error handling; validates graceful degradation | TECH-002 |
| 1.3-INT-011 | Integration | P1 | Verify failed symbols are logged with detailed error messages and symbol list | Observability for debugging fetch failures | TECH-002 |

**Updated Coverage**: 4 tests (3 P0, 1 P1) - Now comprehensive

### Medium Risks (Score 4)

#### TECH-001: Cache Path Collision
**Mitigation Tests**:
- 1.3-UNIT-005 (P0): Deterministic path generation

**Coverage**: 1 test (P0) - Sufficient given atomic file operations (implementation detail)

#### DATA-003: Stale Cache Detection
**Mitigation Tests**:
- 1.3-UNIT-010 (P1): Metadata includes created_at timestamp
- 1.3-UNIT-015 (P0): Force-refresh bypasses cache
- 1.3-UNIT-016 (P1): Invalidate function removes cache
- 1.3-INT-004 (P1): Integration test of force-refresh workflow

**Coverage**: 4 tests (1 P0, 3 P1) - Manual cache management supported; automatic TTL deferred

#### DATA-004: Empty DataFrame Caching
**Mitigation Tests**:
- 1.3-UNIT-009 (P0): Rejects empty DataFrames with informative exception

**Coverage**: 1 test (P0) - Direct validation prevents silent failures

### Low Risks (Score 2-3)

#### DATA-002: MultiIndex Preservation Failure
**Mitigation Tests**:
- 1.3-UNIT-008 (P0): Unit validation of MultiIndex structure
- 1.3-INT-009 (P0): Integration validation after Parquet I/O

**Coverage**: 2 tests (both P0) - Low probability but high impact; appropriate coverage

#### PERF-001: Sequential Symbol Fetching Performance
**Mitigation Tests**:
- 1.3-INT-005 (P1): Cache performance baseline
- 1.3-INT-006 (P2): API performance baseline
- 1.3-INT-007 (P1): Speedup measurement

**Coverage**: 3 tests (2 P1, 1 P2) - Documents baseline for future optimization; acceptable

#### OPS-001: Missing Cache Directory Handling
**Mitigation Tests**:
- 1.3-UNIT-002 (P0): Directory creation validation

**Coverage**: 1 test (P0) - Simple validation; implementation handles edge cases

#### TECH-003: Type Annotation Drift
**Mitigation**: Mypy strict mode enforcement in CI/CD (not test-level mitigation)

#### PERF-002: Parquet Compression Trade-offs
**Mitigation Tests**:
- 1.3-INT-003 (P1): Validates snappy compression used

**Coverage**: 1 test (P1) - Configuration validation; acceptable trade-off documented

---

## Complete Test Scenario List (Updated)

### Unit Tests (18 scenarios)

| ID | Priority | Component | Test Description |
|----|----------|-----------|------------------|
| 1.3-UNIT-001 | P0 | loader.py | Fetch single symbol via bridge on cache miss |
| 1.3-UNIT-002 | P0 | loader.py | Create cache directories if missing |
| 1.3-UNIT-003 | P1 | loader.py | Log progress for multi-symbol fetches |
| 1.3-UNIT-004 | P0 | loader.py | Pass TOTALRETURN adjustment to bridge |
| 1.3-UNIT-005 | P0 | cache.py | Generate consistent cache paths |
| 1.3-UNIT-006 | P0 | cache.py | Validate schema - required columns |
| 1.3-UNIT-007 | P0 | cache.py | Validate schema - correct dtypes |
| 1.3-UNIT-008 | P0 | cache.py | Validate MultiIndex structure |
| 1.3-UNIT-009 | P0 | cache.py | Reject empty DataFrames |
| 1.3-UNIT-010 | P1 | cache.py | Include metadata in Parquet |
| 1.3-UNIT-011 | P0 | cache.py | Return None on cache miss |
| 1.3-UNIT-012 | P0 | cache.py | Return DataFrame on cache hit |
| 1.3-UNIT-013 | P0 | loader.py | Call bridge only on cache miss |
| 1.3-UNIT-014 | P0 | loader.py | Return cached data on cache hit |
| 1.3-UNIT-015 | P0 | loader.py | Force-refresh bypasses cache |
| 1.3-UNIT-016 | P1 | cache.py | Invalidate removes cache files |
| 1.3-UNIT-017 | P0 | cache.py | Round-trip equality (save → load) |
| 1.3-UNIT-018 | P0 | loader.py | Handle partial fetch failures gracefully |

### Integration Tests (8 scenarios)

| ID | Priority | Component | Test Description |
|----|----------|-----------|------------------|
| 1.3-INT-001 | P0 | Full Stack | Fetched data includes dividend column (TOTALRETURN) |
| 1.3-INT-002 | P0 | cache.py | Write Parquet to correct directory path |
| 1.3-INT-003 | P1 | cache.py | Parquet uses snappy compression and pyarrow |
| 1.3-INT-004 | P1 | Full Stack | Force-refresh fetches and overwrites cache |
| 1.3-INT-005 | P1 | Full Stack | Cache load time <50ms for 10 symbols |
| 1.3-INT-006 | P2 | Full Stack | API fetch time ~500ms for 10 symbols |
| 1.3-INT-007 | P1 | Full Stack | Cache speedup >10x vs API |
| 1.3-INT-008 | P0 | cache.py | Loaded DataFrame has correct dtypes after I/O |
| 1.3-INT-009 | P0 | cache.py | MultiIndex preserved after Parquet I/O |
| 1.3-INT-010 | P0 | Full Stack | Cache consistency with partial fetch failures |
| 1.3-INT-011 | P1 | loader.py | Failed symbols logged with error details |

**Updated Total**: 18 unit tests, 11 integration tests = **29 total scenarios**

---

## Test Execution Strategy

### Phase 1: P0 Unit Tests (Fail Fast)
**Execution Time**: <5 seconds
**Scenarios**: 13 tests (UNIT-001, 002, 004, 005, 006, 007, 008, 009, 011, 012, 013, 014, 015, 017, 018)

**Rationale**: Fast feedback on core logic; catches schema validation, caching orchestration, and error handling issues before expensive integration tests

**Failure Response**: Block all further testing until P0 unit tests pass

### Phase 2: P0 Integration Tests
**Execution Time**: <30 seconds
**Scenarios**: 5 tests (INT-001, 002, 008, 009, 010)

**Rationale**: Validates actual file I/O, dtype preservation, and real-world error scenarios

**Failure Response**: Critical defects; fix before P1 tests

### Phase 3: P1 Tests (All Levels)
**Execution Time**: <60 seconds
**Scenarios**: 8 tests (UNIT-003, 010, 016; INT-003, 004, 005, 007, 011)

**Rationale**: Important functionality (metadata, force-refresh, performance) but not blocking

**Failure Response**: Address before merge if time permits; acceptable to defer P2

### Phase 4: P2 Tests
**Execution Time**: <10 seconds
**Scenarios**: 1 test (INT-006)

**Rationale**: Performance baseline documentation; informational only

**Failure Response**: Non-blocking; can investigate post-merge

### Recommended Test Execution Command

```bash
# Phase 1: P0 Unit Tests (fail fast)
uv run pytest tests/stories/1.3/unit/ -m "p0 and unit" -v --tb=short

# Phase 2: P0 Integration Tests
uv run pytest tests/stories/1.3/integration/ -m "p0 and integration" -v --tb=short

# Phase 3: P1 Tests (both levels)
uv run pytest tests/stories/1.3/ -m "p1" -v --tb=short

# Phase 4: P2 Tests
uv run pytest tests/stories/1.3/ -m "p2" -v --tb=short

# Full Story Test Suite (all phases)
uv run pytest tests/stories/1.3/ -v

# With Coverage Report
uv run pytest tests/stories/1.3/ --cov=src/momo/data --cov-report=term-missing -v
```

---

## Test Data Requirements

### Mock Data for Unit Tests

**Sample Price DataFrame** (for cache save/load tests):
```python
import pandas as pd
from datetime import datetime, date

dates = pd.date_range('2020-01-01', '2020-01-10', freq='D')
symbols = ['AAPL', 'MSFT', 'GOOGL']

data = {
    'open': [100.0] * 30,
    'high': [105.0] * 30,
    'low': [95.0] * 30,
    'close': [102.0] * 30,
    'volume': [1000000] * 30,
    'unadjusted_close': [102.0] * 30,
    'dividend': [0.0] * 30
}

index = pd.MultiIndex.from_product([dates, symbols], names=['date', 'symbol'])
sample_df = pd.DataFrame(data, index=index)
```

**Invalid Schema DataFrames** (for schema validation tests):
- Missing column: DataFrame without 'dividend' column
- Wrong dtype: 'volume' as float64 instead of int64
- Wrong index: Single index instead of MultiIndex
- Empty: DataFrame with no rows

**Mock Bridge Responses**:
- Successful single-symbol fetch
- Partial failure (2 of 5 symbols fail)
- Complete failure (all symbols fail)
- Bridge exceptions (NDUNotRunningError, NorgateBridgeError)

### Integration Test Data Requirements

**Test Symbols**:
- Valid symbols: AAPL, MSFT, GOOGL (well-known, liquid stocks)
- Invalid symbol: INVALID_TICKER_XYZ (for error testing)
- Delisted symbol: LEHMQ-201203 (Lehman Brothers, for historical testing)

**Date Ranges**:
- Short range: 2020-01-01 to 2020-01-31 (1 month, fast)
- Medium range: 2020-01-01 to 2020-12-31 (1 year, performance baseline)

**Cache Directories**:
- Use pytest `tmp_path` fixture for isolated test cache directories
- Ensures no cross-test pollution
- Enables parallel test execution

---

## Coverage Gaps and Tradeoffs

### Identified Gaps

**None** - All acceptance criteria have comprehensive test coverage with risk-based prioritization.

### Acceptable Tradeoffs

1. **No E2E Tests**: Data infrastructure layer has no user-facing workflows; integration tests provide sufficient end-to-end validation within the data layer

2. **Deferred Batch Fetching Tests**: Story explicitly defers batch optimization to future work; current tests validate single-symbol sequential fetching only

3. **Manual Cache Staleness**: Automatic TTL or staleness detection not implemented in v1.0; tests validate manual refresh mechanisms (`force_refresh`, `invalidate`)

4. **Limited Concurrency Testing**: Path collision risk (TECH-001) mitigated by atomic file operations; concurrent write tests deferred unless production issues arise

### Test Maintenance Considerations

1. **Schema Evolution**: When `data-models.md` schema changes, update:
   - Sample DataFrames in unit tests
   - Schema validation logic tests (UNIT-006, 007, 008)
   - Metadata schema_version value

2. **Bridge API Changes**: If `bridge.py` interface evolves, update:
   - Mock bridge responses in unit tests
   - Bridge integration tests (UNIT-001, 004, 018)

3. **Performance Thresholds**: If hardware changes affect baselines:
   - Update AC6 thresholds in integration tests
   - Document new baselines in test docstrings

---

## Test Implementation Guidelines

### File Naming Convention

Following ADR-010 (Story-Based Test Organization):

```
tests/stories/1.3/unit/test_1_3_unit_001.py
tests/stories/1.3/unit/test_1_3_unit_002.py
...
tests/stories/1.3/integration/test_1_3_int_001.py
tests/stories/1.3/integration/test_1_3_int_002.py
```

**One test ID per file** (though multiple test functions allowed within file for related scenarios)

### Test Function Requirements

All test functions MUST include:

1. **Pytest Markers** (both required):
   ```python
   @pytest.mark.p0  # or p1, p2
   @pytest.mark.unit  # or integration
   ```

2. **Docstring with Metadata**:
   ```python
   """
   Test ID: 1.3-UNIT-001
   Story: 1.3 - Implement Data Loading and Parquet Caching
   Priority: P0
   Test Level: Unit
   Risk Coverage: TECH-002

   Verifies load_universe() fetches single symbol via bridge when cache misses.
   """
   ```

3. **Type Annotations**: All test functions must pass mypy strict mode

### Fixture Strategy

**Global Fixtures** (`tests/conftest.py`):
- Existing global fixtures (if any)

**Story Fixtures** (`tests/stories/1.3/conftest.py`):
- `sample_price_df`: Standard valid DataFrame for cache tests
- `invalid_schema_dfs`: Collection of schema-violating DataFrames
- `mock_bridge`: Mock bridge with configurable responses
- `temp_cache_dir`: Temporary cache directory using `tmp_path`

**Test-Specific Fixtures**:
- Inline fixtures for unique test scenarios

### Mocking Strategy

**Unit Tests**:
- Mock `cache.load_prices()` and `cache.save_prices()` when testing `loader.py`
- Mock `bridge.fetch_price_data()` for all bridge interactions
- Mock filesystem operations (`Path.mkdir`, `Path.exists`)
- Use `unittest.mock.patch` for mocking

**Integration Tests**:
- Use actual filesystem with `tmp_path` fixture
- Mock bridge calls only when testing cache I/O in isolation
- Use real bridge calls for full-stack integration tests (with appropriate markers for NDU dependency)

---

## Quality Assurance Checklist

Before finalizing test implementation, verify:

- [x] Every acceptance criterion has test coverage
- [x] Test levels are appropriate (unit for logic, integration for I/O)
- [x] No duplicate coverage across levels
- [x] Priorities align with business risk and risk profile
- [x] Test IDs follow naming convention `{epic}.{story}-{LEVEL}-{SEQ}`
- [x] Scenarios are atomic and independent
- [x] High-risk areas (DATA-001, TECH-002) have comprehensive coverage
- [x] Performance thresholds are quantified (AC6: <50ms cache, >10x speedup)
- [x] Error scenarios are tested (partial failures, empty DataFrames, schema violations)
- [x] Metadata and versioning tested for future compatibility

---

## Test Design Metadata for Quality Gate

```yaml
test_design:
  scenarios_total: 29
  by_level:
    unit: 18
    integration: 11
    e2e: 0
  by_priority:
    p0: 16
    p1: 10
    p2: 1
    p3: 0
  coverage_gaps: []
  risk_coverage:
    high_risks_mitigated: 2  # DATA-001, TECH-002
    medium_risks_mitigated: 3  # TECH-001, DATA-003, DATA-004
    low_risks_mitigated: 4  # DATA-002, PERF-001, OPS-001, PERF-002
  execution_estimate:
    p0_unit_seconds: 5
    p0_integration_seconds: 30
    p1_all_seconds: 60
    p2_all_seconds: 10
    total_seconds: 105
```

---

## Trace References

**Test Design Matrix**: `/home/frank/momo/docs/qa/assessments/1.3-test-design-20251204.md`
**P0 Tests Identified**: 16
**P0 Unit Tests**: 13
**P0 Integration Tests**: 3
**Risk Profile Reference**: `/home/frank/momo/docs/qa/assessments/1.3-risk-20251204.md`

---

## Recommendations

### For Development Team

1. **Implement Schema Validation Early**: Build `save_prices()` validation logic before implementing save/load functionality to establish contract
2. **Use TDD for P0 Tests**: Write P0 unit tests first (UNIT-001 through UNIT-018) to drive implementation design
3. **Isolate Bridge Mocking**: Create comprehensive mock bridge in `conftest.py` to enable consistent unit test behavior

### For QA Review

1. **Gate Decision Criteria**: Story can achieve **PASS** gate if all P0 tests implemented and passing
2. **Risk Mitigation Verification**: Specifically verify UNIT-006, 007, 008 (schema validation) and UNIT-018, INT-010 (partial failures) during code review
3. **Performance Baseline**: INT-005, 006, 007 establish baselines for future optimization stories; document results in gate file

### For Future Stories

1. **Schema Version Tracking**: When data models evolve, increment schema_version in metadata and add migration tests
2. **Batch Fetching Optimization**: Performance tests (INT-005 through 007) provide baselines to measure improvement
3. **Staleness Detection**: Current manual approach (force_refresh, invalidate) provides foundation for automatic TTL in future stories

---

## Summary

This test design provides **comprehensive, risk-driven coverage** for Story 1.3's data infrastructure implementation:

- **29 test scenarios** efficiently distributed across unit (62%) and integration (38%) levels
- **55% P0 tests** reflect data integrity criticality in backtesting framework
- **All 10 identified risks** have explicit mitigation tests
- **Zero coverage gaps** across 8 acceptance criteria
- **Fast execution** (~105 seconds total) enables rapid feedback

The design prioritizes **data integrity** (schema validation, MultiIndex preservation) and **robustness** (partial failure handling) while maintaining **efficient test layering** (mock-heavy unit tests, targeted integration tests). This aligns with the foundational nature of the cache layer and positions the codebase for reliable backtesting in future stories.
