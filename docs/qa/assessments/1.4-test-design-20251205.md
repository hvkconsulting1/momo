# Test Design: Story 1.4 - Build Data Quality Validation Pipeline

**Date**: 2025-12-05
**Designer**: Quinn (Test Architect)
**Story**: 1.4 - Build Data Quality Validation Pipeline
**Status**: Draft Story (Pre-Implementation Test Design)

---

## Test Strategy Overview

- **Total test scenarios**: 20
- **Unit tests**: 16 (80%)
- **Integration tests**: 4 (20%)
- **E2E tests**: 0 (0%)
- **Priority distribution**: P0: 9 (45%), P1: 9 (45%), P2: 2 (10%)

### Rationale for Test Distribution

**Unit-Heavy Strategy (80% Unit)**:
Story 1.4 introduces pure validation logic with complex edge cases. The risk profile identifies validation correctness (DATA-001) as the critical risk. Unit tests provide:
- Fast feedback on validation logic correctness
- Isolated testing of each validation helper function
- Easy reproduction of edge cases (market holidays, adjustment errors, delisting scenarios)
- Efficient coverage of multiple data corruption scenarios

**Integration Tests (20%)**:
Integration tests focus on bridge stability (TECH-001 critical risk) and full pipeline validation with cached data. This ensures:
- Real Norgate API calls work as expected
- Bridge error handling and timeouts function correctly
- End-to-end validation pipeline integrates properly

**No E2E Tests**:
Story 1.4 is a data layer module with no user-facing components. All acceptance criteria can be validated through unit and integration tests.

---

## Test Scenarios by Acceptance Criteria

### AC1: `src/data/validation.py` module implements data quality check functions

#### Scenarios

| ID            | Level       | Priority | Test                                                  | Justification                                      | Risk Coverage    |
| ------------- | ----------- | -------- | ----------------------------------------------------- | -------------------------------------------------- | ---------------- |
| 1.4-UNIT-001  | Unit        | P0       | ValidationReport dataclass has all required fields    | Critical data structure for validation results     | DATA-001         |
| 1.4-UNIT-002  | Unit        | P0       | ValidationError exception imports and inherits        | Exception must exist before validation.py imports  | TECH-002         |
| 1.4-UNIT-003  | Unit        | P0       | validate_prices() accepts MultiIndex DataFrame        | Schema compatibility with Story 1.3 loader         | TECH-003         |
| 1.4-UNIT-004  | Unit        | P0       | validate_prices() returns ValidationReport            | Core function contract verification                | DATA-001         |

**Coverage Summary**: These tests verify the foundational structure of the validation module exists and adheres to documented interfaces.

---

### AC2: Validation detects missing price data (NaN or gaps in expected date ranges)

#### Scenarios

| ID            | Level       | Priority | Test                                                  | Justification                                      | Risk Coverage    |
| ------------- | ----------- | -------- | ----------------------------------------------------- | -------------------------------------------------- | ---------------- |
| 1.4-UNIT-005  | Unit        | P0       | validate_prices() with clean data reports no issues   | Baseline validation behavior (no false positives)  | DATA-001         |
| 1.4-UNIT-006  | Unit        | P0       | _check_missing_values() detects NaN in close column   | Critical for price data integrity                  | DATA-001         |
| 1.4-UNIT-007  | Unit        | P0       | _check_missing_values() detects NaN in OHLC columns   | Comprehensive missing data detection               | DATA-001         |
| 1.4-UNIT-008  | Unit        | P1       | _check_date_gaps() detects 10-business-day gap        | Identifies suspicious data gaps                    | DATA-002         |
| 1.4-UNIT-009  | Unit        | P1       | _check_date_gaps() ignores weekend gaps               | Avoids false positives from normal market closures | DATA-002         |
| 1.4-UNIT-010  | Unit        | P1       | _check_date_gaps() ignores known market holidays      | Avoids false positives from NYSE holidays          | DATA-002         |

**Coverage Summary**: Comprehensive coverage of missing data detection with focus on avoiding false positives (DATA-002 high risk).

---

### AC3: Validation checks for adjustment factor consistency (splits/dividends properly applied)

#### Scenarios

| ID            | Level       | Priority | Test                                                  | Justification                                      | Risk Coverage    |
| ------------- | ----------- | -------- | ----------------------------------------------------- | -------------------------------------------------- | ---------------- |
| 1.4-UNIT-011  | Unit        | P0       | _check_adjustment_consistency() flags negative prices | Invalid state after adjustment error               | DATA-001         |
| 1.4-UNIT-012  | Unit        | P1       | _check_adjustment_consistency() flags 50% jump no div | Detects missing split/dividend adjustment          | DATA-004         |
| 1.4-UNIT-013  | Unit        | P1       | _check_adjustment_consistency() allows AAPL 7:1 split | Known historical split should not be flagged       | DATA-004         |

**Coverage Summary**: Adjustment consistency uses heuristics (DATA-004 medium risk). Tests verify thresholds detect real issues without excessive false positives.

---

### AC4: Function retrieves point-in-time index constituents for a specified date

#### Scenarios

| ID            | Level       | Priority | Test                                                      | Justification                                      | Risk Coverage    |
| ------------- | ----------- | -------- | --------------------------------------------------------- | -------------------------------------------------- | ---------------- |
| 1.4-UNIT-014  | Unit        | P0       | get_index_constituents_at_date() with mocked bridge       | Core function contract with controlled test        | TECH-001         |
| 1.4-UNIT-015  | Unit        | P0       | get_index_constituents_at_date() handles bridge timeout   | Graceful degradation on bridge failure             | TECH-001         |
| 1.4-UNIT-016  | Unit        | P1       | get_index_constituents_at_date() raises on invalid index  | Error handling for bad inputs                      | TECH-001         |
| 1.4-INT-001   | Integration | P0       | Fetch Russell 1000 constituents for 2010-01-01 via bridge | Real bridge call validates API integration         | TECH-001, PERF-002 |

**Coverage Summary**: Critical bridge integration (TECH-001 critical risk) tested with both mocked unit tests and real integration test requiring NDU.

---

### AC5: Delisting data is accessible and can be queried

#### Scenarios

| ID            | Level       | Priority | Test                                                  | Justification                                      | Risk Coverage    |
| ------------- | ----------- | -------- | ----------------------------------------------------- | -------------------------------------------------- | ---------------- |
| 1.4-UNIT-017  | Unit        | P1       | check_delisting_status() identifies Enron as delisted | Known delisted ticker (historical validation)      | DATA-003         |
| 1.4-UNIT-018  | Unit        | P1       | check_delisting_status() returns last trading date    | Provides actionable delisting information          | DATA-003         |

**Coverage Summary**: Delisting detection (DATA-003 high risk) validated with known historical delisted ticker.

---

### AC6: Validation report summarizes: total tickers, date range, missing data counts, delisting events

#### Scenarios

| ID            | Level       | Priority | Test                                                  | Justification                                      | Risk Coverage    |
| ------------- | ----------- | -------- | ----------------------------------------------------- | -------------------------------------------------- | ---------------- |
| 1.4-UNIT-019  | Unit        | P1       | ValidationReport has concise summary_message          | Human-readable report for developer UX             | OPS-001          |
| 1.4-UNIT-020  | Unit        | P1       | ValidationReport __str__ method formats cleanly       | Printable report output for quick inspection       | OPS-001          |

**Coverage Summary**: Report usability (OPS-001 medium risk) verified through summary and formatting tests.

---

### AC7: Tests verify validation functions correctly identify synthetic missing data and corporate action issues

#### Scenarios

| ID            | Level       | Priority | Test                                                      | Justification                                      | Risk Coverage    |
| ------------- | ----------- | -------- | --------------------------------------------------------- | -------------------------------------------------- | ---------------- |
| 1.4-INT-002   | Integration | P0       | Full validation pipeline with known-corrupt cached data   | End-to-end validation with realistic failure modes | DATA-001         |
| 1.4-INT-003   | Integration | P1       | Validation performance on 100-ticker universe < 5s        | Performance requirement (< 5s for typical load)    | PERF-001         |

**Coverage Summary**: Integration tests validate full pipeline with realistic data corruption scenarios and performance requirements.

---

### AC8: Documentation explains how to interpret validation reports

#### Scenarios

| ID            | Level       | Priority | Test                                                  | Justification                                      | Risk Coverage    |
| ------------- | ----------- | -------- | ----------------------------------------------------- | -------------------------------------------------- | ---------------- |
| 1.4-UNIT-021  | Unit        | P2       | Module docstring contains usage example               | Documentation completeness check                   | OPS-002          |
| 1.4-INT-004   | Integration | P2       | Manual validation report review for readability       | Human validation of report UX                      | OPS-002          |

**Coverage Summary**: Documentation quality (OPS-002 medium risk) verified through automated docstring check and manual review.

---

## Risk Coverage Matrix

### Critical Risks (Score 9)

| Risk ID  | Risk Description                                   | Test Coverage                                     | Coverage Level |
| -------- | -------------------------------------------------- | ------------------------------------------------- | -------------- |
| DATA-001 | Incomplete/incorrect validation logic              | 1.4-UNIT-001 to 1.4-UNIT-007, 1.4-UNIT-011, 1.4-INT-002 | High (9 tests) |
| TECH-001 | Bridge instability during constituent retrieval    | 1.4-UNIT-014, 1.4-UNIT-015, 1.4-UNIT-016, 1.4-INT-001 | High (4 tests) |

**Assessment**: Both critical risks have comprehensive test coverage with P0 priority tests at unit and integration levels.

---

### High Risks (Score 6)

| Risk ID  | Risk Description                                   | Test Coverage                                     | Coverage Level |
| -------- | -------------------------------------------------- | ------------------------------------------------- | -------------- |
| DATA-002 | False positives in date gap detection              | 1.4-UNIT-008, 1.4-UNIT-009, 1.4-UNIT-010          | Medium (3 tests) |
| PERF-001 | Validation performance degrades with large data    | 1.4-INT-003                                       | Low (1 test)   |
| TECH-002 | ValidationError exception not yet defined          | 1.4-UNIT-002                                      | Low (1 test)   |
| DATA-003 | Delisting detection edge cases                     | 1.4-UNIT-017, 1.4-UNIT-018                        | Low (2 tests)  |

**Assessment**: High risks have targeted test coverage. PERF-001 relies on integration profiling; others have unit test validation.

---

### Medium Risks (Score 4)

| Risk ID  | Risk Description                                   | Test Coverage                                     | Coverage Level |
| -------- | -------------------------------------------------- | ------------------------------------------------- | -------------- |
| TECH-003 | MultiIndex preservation in validation              | 1.4-UNIT-003                                      | Low (1 test)   |
| DATA-004 | Adjustment consistency heuristics                  | 1.4-UNIT-012, 1.4-UNIT-013                        | Medium (2 tests) |
| OPS-001  | Validation reports not human-readable              | 1.4-UNIT-019, 1.4-UNIT-020                        | Medium (2 tests) |
| TECH-004 | Complex type hints for nested structures           | (Type checking via mypy, no explicit test)        | N/A            |
| DATA-005 | Cached validation results become stale             | (Risk mitigated by not implementing caching)      | N/A            |
| OPS-002  | Missing usage examples in documentation            | 1.4-UNIT-021, 1.4-INT-004                         | Low (2 tests)  |

**Assessment**: Medium risks have adequate coverage. TECH-004 relies on CI mypy checks. DATA-005 risk eliminated by not implementing caching.

---

### Low Risks (Score 2-3)

| Risk ID  | Risk Description                                   | Mitigation                                        | Test Coverage  |
| -------- | -------------------------------------------------- | ------------------------------------------------- | -------------- |
| BUS-001  | Unfixable historical data quality issues           | Accepted risk, documented limitation              | N/A            |
| PERF-002 | Bridge call for index constituents slow            | Integration test measures timing                  | 1.4-INT-001    |
| SEC-001  | Validation report exposes ticker info in logs      | Accepted for research tool                        | N/A            |

**Assessment**: Low risks accepted or monitored via integration tests. No additional test coverage required.

---

## Test Pyramid Compliance

| Level       | Scenario Count | Percentage | Target  | Status     |
| ----------- | -------------- | ---------- | ------- | ---------- |
| Unit        | 16             | 80%        | 70%     | ✅ Compliant |
| Integration | 4              | 20%        | 25%     | ✅ Compliant |
| E2E         | 0              | 0%         | 5%      | ✅ N/A (no UI) |

**Assessment**: Test distribution aligns with project test pyramid targets. Unit-heavy approach appropriate for validation logic.

---

## Detailed Test Specifications

### Unit Tests

#### 1.4-UNIT-001: ValidationReport dataclass has all required fields

**Priority**: P0
**Risk Coverage**: DATA-001
**Test Level**: Unit

**Given**: ValidationReport dataclass is defined in validation.py
**When**: I instantiate a ValidationReport with all required fields
**Then**: The dataclass has fields: total_tickers, date_range, missing_data_counts, date_gaps, adjustment_issues, delisting_events, summary_message, is_valid

**Test Data**:
```python
report = ValidationReport(
    total_tickers=10,
    date_range=(date(2020, 1, 1), date(2021, 1, 1)),
    missing_data_counts={"AAPL": 5},
    date_gaps={"MSFT": [(date(2020, 6, 1), date(2020, 6, 10))]},
    adjustment_issues=["TSLA"],
    delisting_events={"ENRN": date(2001, 12, 2)},
    summary_message="Validation complete: 1 issue found",
    is_valid=False
)
```

**Assertions**:
- All fields are accessible via dot notation
- Type hints match expected types (verified by mypy)

---

#### 1.4-UNIT-002: ValidationError exception imports and inherits

**Priority**: P0
**Risk Coverage**: TECH-002
**Test Level**: Unit

**Given**: ValidationError is defined in src/momo/utils/exceptions.py
**When**: I import ValidationError
**Then**: ValidationError is a subclass of DataError

**Test Code**:
```python
from momo.utils.exceptions import ValidationError, DataError

assert issubclass(ValidationError, DataError)
try:
    raise ValidationError("Test validation failure")
except DataError:
    pass  # Should catch as DataError parent
```

---

#### 1.4-UNIT-003: validate_prices() accepts MultiIndex DataFrame

**Priority**: P0
**Risk Coverage**: TECH-003
**Test Level**: Unit

**Given**: A prices DataFrame with MultiIndex (date, symbol) from Story 1.3 loader
**When**: I call validate_prices(prices_df)
**Then**: The function executes without error and input DataFrame MultiIndex is preserved

**Test Data**: Fixture `sample_price_df_clean` (5 tickers, 10 days, MultiIndex)

**Assertions**:
- validate_prices() returns ValidationReport
- Input DataFrame still has MultiIndex (date, symbol) after function call
- Input DataFrame data unchanged (no mutation)

---

#### 1.4-UNIT-004: validate_prices() returns ValidationReport

**Priority**: P0
**Risk Coverage**: DATA-001
**Test Level**: Unit

**Given**: A clean prices DataFrame
**When**: I call validate_prices(prices_df)
**Then**: The return value is a ValidationReport instance with is_valid=True

**Test Data**: Fixture `sample_price_df_clean`

**Assertions**:
- isinstance(result, ValidationReport) is True
- result.is_valid is True
- result.summary_message contains "no issues" or similar

---

#### 1.4-UNIT-005: validate_prices() with clean data reports no issues

**Priority**: P0
**Risk Coverage**: DATA-001
**Test Level**: Unit

**Given**: A prices DataFrame with complete data (no NaN, no gaps, valid adjustments)
**When**: I call validate_prices(prices_df)
**Then**: ValidationReport shows: missing_data_counts={}, date_gaps={}, adjustment_issues=[], is_valid=True

**Test Data**: Fixture `sample_price_df_clean`

**Assertions**:
- len(result.missing_data_counts) == 0
- len(result.date_gaps) == 0
- len(result.adjustment_issues) == 0
- result.is_valid is True

---

#### 1.4-UNIT-006: _check_missing_values() detects NaN in close column

**Priority**: P0
**Risk Coverage**: DATA-001
**Test Level**: Unit

**Given**: A prices DataFrame with NaN values in 'close' column for ticker AAPL
**When**: I call _check_missing_values(prices_df)
**Then**: The function returns {"AAPL": N} where N is the count of NaN values

**Test Data**: Fixture `sample_price_df_with_nans_close` (AAPL has 3 NaN in close)

**Assertions**:
- result["AAPL"] == 3
- Other tickers not in result (no missing data)

---

#### 1.4-UNIT-007: _check_missing_values() detects NaN in OHLC columns

**Priority**: P0
**Risk Coverage**: DATA-001
**Test Level**: Unit

**Given**: A prices DataFrame with NaN values in open, high, low columns for ticker MSFT
**When**: I call _check_missing_values(prices_df)
**Then**: The function returns {"MSFT": N} aggregating NaN across OHLC columns

**Test Data**: Fixture `sample_price_df_with_nans_ohlc` (MSFT has 2 NaN in open, 1 NaN in high)

**Assertions**:
- result["MSFT"] >= 3 (at least 3 total NaN values detected)
- Function checks all OHLC columns, not just close

---

#### 1.4-UNIT-008: _check_date_gaps() detects 10-business-day gap

**Priority**: P1
**Risk Coverage**: DATA-002
**Test Level**: Unit

**Given**: A prices DataFrame with a 10-business-day gap for ticker TSLA (e.g., 2020-06-01 to 2020-06-15)
**When**: I call _check_date_gaps(prices_df)
**Then**: The function returns {"TSLA": [(date(2020-06-01), date(2020-06-15))]}

**Test Data**: Fixture `sample_price_df_with_10day_gap`

**Assertions**:
- "TSLA" in result
- Gap dates are correctly identified
- Gap span is >= 10 business days

---

#### 1.4-UNIT-009: _check_date_gaps() ignores weekend gaps

**Priority**: P1
**Risk Coverage**: DATA-002
**Test Level**: Unit

**Given**: A prices DataFrame with continuous weekday data (no weekend price data)
**When**: I call _check_date_gaps(prices_df)
**Then**: The function returns {} (no gaps detected for weekend dates)

**Test Data**: Fixture `sample_price_df_with_weekends_missing` (typical trading data)

**Assertions**:
- len(result) == 0
- Weekend gaps do not trigger false positives

---

#### 1.4-UNIT-010: _check_date_gaps() ignores known market holidays

**Priority**: P1
**Risk Coverage**: DATA-002
**Test Level**: Unit

**Given**: A prices DataFrame missing data for 2020-07-03 (Independence Day observed)
**When**: I call _check_date_gaps(prices_df)
**Then**: The function returns {} (no gaps detected for known market holiday)

**Test Data**: Fixture `sample_price_df_with_july4_missing`

**Assertions**:
- len(result) == 0
- Market holiday does not trigger false positive

**Note**: If `pandas_market_calendars` library is not used, document this limitation in ValidationReport.

---

#### 1.4-UNIT-011: _check_adjustment_consistency() flags negative prices

**Priority**: P0
**Risk Coverage**: DATA-001
**Test Level**: Unit

**Given**: A prices DataFrame with negative 'close' values for ticker FAIL
**When**: I call _check_adjustment_consistency(prices_df)
**Then**: The function returns ["FAIL"] (ticker flagged for invalid adjustment)

**Test Data**: Fixture `sample_price_df_with_negative_price`

**Assertions**:
- "FAIL" in result
- Other tickers not flagged

---

#### 1.4-UNIT-012: _check_adjustment_consistency() flags 50% jump without dividend

**Priority**: P1
**Risk Coverage**: DATA-004
**Test Level**: Unit

**Given**: A prices DataFrame where ticker XYZ has 50% price jump without corresponding dividend entry
**When**: I call _check_adjustment_consistency(prices_df)
**Then**: The function returns ["XYZ"] (ticker flagged for suspicious price jump)

**Test Data**: Fixture `sample_price_df_with_50pct_jump_no_div`
```python
# Day 1: close = 100
# Day 2: close = 150 (50% jump)
# dividend column = 0.0 for both days
```

**Assertions**:
- "XYZ" in result
- Heuristic threshold correctly identifies suspicious jump

---

#### 1.4-UNIT-013: _check_adjustment_consistency() allows AAPL 7:1 split

**Priority**: P1
**Risk Coverage**: DATA-004
**Test Level**: Unit

**Given**: A prices DataFrame with Apple's historical 7:1 stock split (June 9, 2014)
**When**: I call _check_adjustment_consistency(prices_df)
**Then**: The function returns [] (no issues flagged for known legitimate split)

**Test Data**: Fixture `sample_price_df_with_aapl_split` (historical Apple data around June 2014)

**Assertions**:
- "AAPL" not in result
- Known corporate actions do not trigger false positives

**Note**: This test may require real or realistic synthetic data with proper adjustment factors.

---

#### 1.4-UNIT-014: get_index_constituents_at_date() with mocked bridge

**Priority**: P0
**Risk Coverage**: TECH-001
**Test Level**: Unit

**Given**: A mocked bridge response for Russell 1000 constituents on 2010-01-01
**When**: I call get_index_constituents_at_date("Russell 1000 Current & Past", date(2010, 1, 1))
**Then**: The function returns a list of ticker symbols (mock data)

**Test Data**: Mock fixture `mock_bridge_response_russell1000` (returns ["AAPL", "MSFT", ...])

**Assertions**:
- Result is a list of strings (ticker symbols)
- len(result) > 0
- Bridge called exactly once with correct parameters

---

#### 1.4-UNIT-015: get_index_constituents_at_date() handles bridge timeout

**Priority**: P0
**Risk Coverage**: TECH-001
**Test Level**: Unit

**Given**: A mocked bridge call that raises TimeoutError
**When**: I call get_index_constituents_at_date("Russell 1000 Current & Past", date(2010, 1, 1))
**Then**: The function logs a warning and raises BridgeError with graceful error message

**Test Code**:
```python
with patch("momo.data.bridge.execute_norgate_code") as mock_bridge:
    mock_bridge.side_effect = TimeoutError("Bridge timeout")
    with pytest.raises(BridgeError) as exc_info:
        get_index_constituents_at_date("Russell 1000", date(2010, 1, 1))
    assert "timeout" in str(exc_info.value).lower()
```

**Assertions**:
- BridgeError raised with descriptive message
- Error logged via structlog with context (index name, date)

---

#### 1.4-UNIT-016: get_index_constituents_at_date() raises on invalid index

**Priority**: P1
**Risk Coverage**: TECH-001
**Test Level**: Unit

**Given**: A bridge call with invalid index name "InvalidIndex123"
**When**: I call get_index_constituents_at_date("InvalidIndex123", date(2010, 1, 1))
**Then**: The function raises ValueError with message "Invalid index name"

**Test Code**:
```python
with pytest.raises(ValueError) as exc_info:
    get_index_constituents_at_date("InvalidIndex123", date(2010, 1, 1))
assert "invalid" in str(exc_info.value).lower()
```

---

#### 1.4-UNIT-017: check_delisting_status() identifies Enron as delisted

**Priority**: P1
**Risk Coverage**: DATA-003
**Test Level**: Unit

**Given**: A prices DataFrame with Enron (ENRN) data ending on 2001-12-02
**When**: I call check_delisting_status(prices_df, query_end_date=date(2020, 1, 1))
**Then**: The function returns {"ENRN": date(2001, 12, 2)}

**Test Data**: Fixture `sample_price_df_with_enron_delisted`

**Assertions**:
- "ENRN" in result
- result["ENRN"] == date(2001, 12, 2) (last trading date)

---

#### 1.4-UNIT-018: check_delisting_status() returns last trading date

**Priority**: P1
**Risk Coverage**: DATA-003
**Test Level**: Unit

**Given**: A prices DataFrame with ticker ABC delisted on 2019-06-15
**When**: I call check_delisting_status(prices_df, query_end_date=date(2020, 1, 1))
**Then**: The function returns {"ABC": date(2019, 6, 15)}

**Test Data**: Fixture `sample_price_df_with_recent_delisting`

**Assertions**:
- result["ABC"] is the last date in ABC's price time series
- Delisting heuristic: data ends > 30 days before query_end_date

---

#### 1.4-UNIT-019: ValidationReport has concise summary_message

**Priority**: P1
**Risk Coverage**: OPS-001
**Test Level**: Unit

**Given**: A ValidationReport with 5 tickers with missing data, 2 with adjustment issues, 10 delisted
**When**: I access report.summary_message
**Then**: The message is concise (< 200 chars) and human-readable

**Test Data**: ValidationReport fixture with specific issues

**Assertions**:
- len(report.summary_message) < 200
- Message contains key numbers: "5 tickers with missing data"
- Message is grammatically correct and readable

---

#### 1.4-UNIT-020: ValidationReport __str__ method formats cleanly

**Priority**: P1
**Risk Coverage**: OPS-001
**Test Level**: Unit

**Given**: A ValidationReport instance
**When**: I call str(report) or print(report)
**Then**: The output is formatted with clear section headers and readable structure

**Test Code**:
```python
report = ValidationReport(...)
output = str(report)
assert "Total Tickers:" in output
assert "Date Range:" in output
assert "Validation Status:" in output
```

**Assertions**:
- Output contains section headers
- Nested structures (dicts, lists) are formatted readably
- Summary appears first or last for quick scanning

---

#### 1.4-UNIT-021: Module docstring contains usage example

**Priority**: P2
**Risk Coverage**: OPS-002
**Test Level**: Unit

**Given**: The validation.py module
**When**: I read the module's __doc__ attribute
**Then**: The docstring contains a usage example with validate_prices()

**Test Code**:
```python
import momo.data.validation as validation_module
docstring = validation_module.__doc__
assert "Example:" in docstring or "Usage:" in docstring
assert "validate_prices" in docstring
```

**Assertions**:
- Docstring exists and is non-empty
- Contains usage example code
- Example shows typical validation workflow

---

### Integration Tests

#### 1.4-INT-001: Fetch Russell 1000 constituents for 2010-01-01 via bridge

**Priority**: P0
**Risk Coverage**: TECH-001, PERF-002
**Test Level**: Integration

**Given**: Norgate Data Updater (NDU) is running and accessible via Windows Python bridge
**When**: I call get_index_constituents_at_date("Russell 1000 Current & Past", date(2010, 1, 1))
**Then**: The function returns a list of ~1000 ticker symbols in < 60s

**Test Requirements**:
- **Requires NDU**: Mark with `@pytest.mark.integration` and skip if NDU unavailable
- **Timeout**: 60s maximum
- **Assertions**:
  - len(result) > 900 (allow for some variance in constituent count)
  - len(result) < 1100
  - result contains expected tickers (e.g., "AAPL", "MSFT")
  - Execution time < 60s (logged via structlog)

**Skip Condition**:
```python
@pytest.mark.integration
@pytest.mark.skipif(not ndu_available(), reason="NDU not running")
def test_1_4_int_001():
    ...
```

---

#### 1.4-INT-002: Full validation pipeline with known-corrupt cached data

**Priority**: P0
**Risk Coverage**: DATA-001
**Test Level**: Integration

**Given**: A cached Parquet file with known data corruption (NaN values, date gaps, adjustment errors)
**When**: I call validate_prices() on the loaded DataFrame
**Then**: ValidationReport correctly identifies all synthetic issues with is_valid=False

**Test Data**: Pre-generated cached test data:
- `test_data_corrupt.parquet` with:
  - AAPL: 5 NaN values in close column
  - MSFT: 10-day date gap (2020-06-01 to 2020-06-15)
  - TSLA: 50% price jump without dividend
  - ENRN: Delisted (data ends 2001-12-02)

**Assertions**:
- result.is_valid is False
- "AAPL" in result.missing_data_counts with count == 5
- "MSFT" in result.date_gaps
- "TSLA" in result.adjustment_issues
- "ENRN" in result.delisting_events
- result.summary_message mentions all 4 issues

---

#### 1.4-INT-003: Validation performance on 100-ticker universe < 5s

**Priority**: P1
**Risk Coverage**: PERF-001
**Test Level**: Integration

**Given**: A prices DataFrame with 100 tickers, 252 trading days (1 year of data)
**When**: I call validate_prices(prices_df)
**Then**: Execution completes in < 5 seconds

**Test Data**: Fixture `sample_price_df_100tickers` (generated synthetic data or cached real data)

**Assertions**:
- Execution time < 5.0 seconds (measured with `time.time()`)
- ValidationReport returned successfully
- Log output shows performance metrics

**Test Code**:
```python
import time
start = time.time()
result = validate_prices(prices_df_100tickers)
elapsed = time.time() - start
assert elapsed < 5.0, f"Validation took {elapsed:.2f}s, expected < 5s"
```

---

#### 1.4-INT-004: Manual validation report review for readability

**Priority**: P2
**Risk Coverage**: OPS-002
**Test Level**: Integration

**Given**: A ValidationReport from a realistic dataset
**When**: I print the report to console
**Then**: Manual review confirms the output is readable and actionable

**Test Process**:
1. Run validation on cached test data
2. Print ValidationReport using `print(str(report))`
3. Manual developer review:
   - [ ] Summary is visible at top or bottom
   - [ ] Sections are clearly delineated
   - [ ] Nested structures are not overwhelming
   - [ ] Key metrics are easy to find
   - [ ] Report provides actionable information

**Note**: This is a semi-automated test requiring human validation. Document findings in test output.

---

## Test Fixtures

### Story-Level Fixtures (tests/stories/1.4/conftest.py)

```python
@pytest.fixture
def sample_price_df_clean() -> pd.DataFrame:
    """Clean prices DataFrame with 5 tickers, 10 days, no issues."""
    # MultiIndex (date, symbol), complete OHLC data, no NaN, no gaps
    ...

@pytest.fixture
def sample_price_df_with_nans_close() -> pd.DataFrame:
    """Prices DataFrame with 3 NaN values in AAPL close column."""
    ...

@pytest.fixture
def sample_price_df_with_nans_ohlc() -> pd.DataFrame:
    """Prices DataFrame with NaN values across OHLC columns for MSFT."""
    ...

@pytest.fixture
def sample_price_df_with_10day_gap() -> pd.DataFrame:
    """Prices DataFrame with 10-business-day gap for TSLA."""
    ...

@pytest.fixture
def sample_price_df_with_weekends_missing() -> pd.DataFrame:
    """Typical trading data with no weekend prices (expected behavior)."""
    ...

@pytest.fixture
def sample_price_df_with_july4_missing() -> pd.DataFrame:
    """Trading data missing July 4, 2020 (Independence Day observed on July 3)."""
    ...

@pytest.fixture
def sample_price_df_with_negative_price() -> pd.DataFrame:
    """Prices DataFrame with negative close price for ticker FAIL."""
    ...

@pytest.fixture
def sample_price_df_with_50pct_jump_no_div() -> pd.DataFrame:
    """Prices DataFrame with 50% price jump without dividend for XYZ."""
    ...

@pytest.fixture
def sample_price_df_with_aapl_split() -> pd.DataFrame:
    """Historical Apple data with 7:1 stock split on June 9, 2014."""
    ...

@pytest.fixture
def sample_price_df_with_enron_delisted() -> pd.DataFrame:
    """Prices DataFrame with Enron data ending December 2, 2001."""
    ...

@pytest.fixture
def sample_price_df_with_recent_delisting() -> pd.DataFrame:
    """Prices DataFrame with ticker ABC delisted on 2019-06-15."""
    ...

@pytest.fixture
def sample_price_df_100tickers() -> pd.DataFrame:
    """Large dataset with 100 tickers, 252 trading days for performance testing."""
    ...

@pytest.fixture
def mock_bridge_response_russell1000() -> list[str]:
    """Mock bridge response with ~1000 ticker symbols."""
    return ["AAPL", "MSFT", "GOOGL", ...]  # 1000 tickers

@pytest.fixture
def mock_validation_report() -> ValidationReport:
    """Sample ValidationReport for testing."""
    ...
```

---

## Test Execution Strategy

### Recommended Execution Order

1. **Phase 1: P0 Unit Tests (Fast Fail)**
   - Run: `uv run pytest tests/stories/1.4/unit/ -m p0 -v`
   - Duration: < 1s
   - Purpose: Validate core validation logic and data structures
   - Gate: Must pass 100% before proceeding

2. **Phase 2: P1 Unit Tests (Edge Cases)**
   - Run: `uv run pytest tests/stories/1.4/unit/ -m p1 -v`
   - Duration: < 2s
   - Purpose: Validate edge case handling and heuristics
   - Gate: Should pass 100% before merge

3. **Phase 3: P0 Integration Tests (Bridge & Pipeline)**
   - Run: `uv run pytest tests/stories/1.4/integration/ -m p0 -v`
   - Duration: < 70s (bridge call + pipeline test)
   - Purpose: Validate real Norgate API integration
   - Gate: Must pass 100% before merge (or skip if NDU unavailable with documentation)

4. **Phase 4: P1/P2 Tests (Nice to Have)**
   - Run: `uv run pytest tests/stories/1.4/ -m "p1 or p2" -v`
   - Duration: < 10s
   - Purpose: Performance, documentation, usability validation
   - Gate: Should pass, but not blocking

### CI/CD Integration

**Pre-commit Hook**:
```bash
# Type checking
uv run mypy src/momo/data/validation.py

# Fast unit tests
uv run pytest tests/stories/1.4/unit/ -m p0 -v
```

**Pull Request Gate**:
```bash
# All tests except NDU-dependent integration tests
uv run pytest tests/stories/1.4/ -m "not integration or not requires_ndu" -v

# Type checking
uv run mypy src/momo/data/validation.py tests/stories/1.4/

# Linting
uv run ruff check src/momo/data/validation.py
```

**Full Test Suite** (Manual or Nightly):
```bash
# All tests including NDU integration
uv run pytest tests/stories/1.4/ -v --cov=src/momo/data/validation --cov-report=term-missing
```

---

## Coverage Gaps Analysis

### Acceptance Criteria Coverage

| AC | Description                                      | Test Coverage                       | Gap Analysis        |
| -- | ------------------------------------------------ | ----------------------------------- | ------------------- |
| 1  | validation.py module implements functions        | 1.4-UNIT-001 to 004                 | ✅ Full coverage     |
| 2  | Detects missing price data                       | 1.4-UNIT-005 to 010                 | ✅ Full coverage     |
| 3  | Checks adjustment factor consistency             | 1.4-UNIT-011 to 013                 | ✅ Full coverage     |
| 4  | Retrieves point-in-time index constituents       | 1.4-UNIT-014 to 016, 1.4-INT-001    | ✅ Full coverage     |
| 5  | Delisting data accessible and queryable          | 1.4-UNIT-017, 018                   | ✅ Full coverage     |
| 6  | Validation report summarizes required fields     | 1.4-UNIT-019, 020                   | ✅ Full coverage     |
| 7  | Tests verify validation functions                | All unit + 1.4-INT-002, 003         | ✅ Full coverage     |
| 8  | Documentation explains report interpretation     | 1.4-UNIT-021, 1.4-INT-004           | ✅ Full coverage     |

**Assessment**: All 8 acceptance criteria have test coverage. No gaps identified.

---

### Risk Coverage Gaps

| Risk Priority | Total Risks | Tested Risks | Untested Risks | Gap Assessment                |
| ------------- | ----------- | ------------ | -------------- | ----------------------------- |
| Critical (9)  | 2           | 2            | 0              | ✅ Full coverage               |
| High (6)      | 4           | 4            | 0              | ✅ Full coverage               |
| Medium (4)    | 6           | 4            | 2              | ⚠️ TECH-004, DATA-005 deferred |
| Low (2-3)     | 3           | 1            | 2              | ✅ Acceptable gaps             |

**Gap Details**:

1. **TECH-004 (Complex type hints)**: Mitigated by mypy CI checks, no explicit test needed
2. **DATA-005 (Cached validation results stale)**: Risk eliminated by not implementing caching in Story 1.4
3. **BUS-001 (Unfixable data issues)**: Accepted risk, no test needed
4. **SEC-001 (Log exposure)**: Accepted risk for research tool, no test needed

**Assessment**: Coverage gaps are intentional and documented. No additional tests required.

---

## Quality Checklist

Before finalizing test design, verify:

- [x] Every AC has test coverage
- [x] Test levels are appropriate (not over-testing)
- [x] No duplicate coverage across levels
- [x] Priorities align with business risk
- [x] Test IDs follow naming convention (1.4-{LEVEL}-{SEQ})
- [x] Scenarios are atomic and independent
- [x] Critical risks have P0 test coverage
- [x] Integration tests marked appropriately (NDU dependency)
- [x] Performance requirements specified (< 5s for 100 tickers)
- [x] Test pyramid compliance (80% unit, 20% integration)

---

## Recommended Test Implementation Order

### Sprint 1: Foundation (P0 Critical Path)

1. **Story fixtures**: Create conftest.py with core fixtures (clean data, NaN data, gap data)
2. **1.4-UNIT-002**: ValidationError exception (dependency for all other tests)
3. **1.4-UNIT-001, 003, 004**: ValidationReport structure and validate_prices() contract
4. **1.4-UNIT-005, 006, 007**: Missing data detection (critical risk DATA-001)
5. **1.4-UNIT-011**: Negative price detection (critical risk DATA-001)
6. **1.4-INT-002**: Full validation pipeline with corrupt data (critical risk DATA-001)

**Gate**: Must complete Sprint 1 before proceeding to implementation. These tests define the core validation contract.

---

### Sprint 2: Bridge Integration (P0 Critical Path)

1. **1.4-UNIT-014, 015, 016**: Bridge integration with mocked responses (critical risk TECH-001)
2. **1.4-INT-001**: Real bridge call to fetch Russell 1000 constituents (critical risk TECH-001)

**Gate**: Must complete Sprint 2 to validate bridge stability before relying on constituent retrieval.

---

### Sprint 3: Edge Cases & Heuristics (P1 High Priority)

1. **1.4-UNIT-008, 009, 010**: Date gap detection edge cases (high risk DATA-002)
2. **1.4-UNIT-012, 013**: Adjustment consistency heuristics (medium risk DATA-004)
3. **1.4-UNIT-017, 018**: Delisting detection (high risk DATA-003)
4. **1.4-INT-003**: Performance validation (high risk PERF-001)

**Gate**: Should complete Sprint 3 for production-ready validation. These tests prevent false positives and ensure performance.

---

### Sprint 4: Usability & Documentation (P1/P2 Nice to Have)

1. **1.4-UNIT-019, 020**: ValidationReport usability (medium risk OPS-001)
2. **1.4-UNIT-021**: Documentation completeness (medium risk OPS-002)
3. **1.4-INT-004**: Manual report readability review (medium risk OPS-002)

**Gate**: Complete Sprint 4 for developer experience. These tests ensure validation is usable and well-documented.

---

## Test Maintenance Strategy

### Fixture Maintenance

- **Location**: `tests/stories/1.4/conftest.py`
- **Responsibility**: Test fixtures should be DRY and reusable across multiple tests
- **Evolution**: As validation logic evolves, update fixtures to reflect new test needs
- **Documentation**: Each fixture should have clear docstring explaining data characteristics

### Test Data Management

- **Synthetic Data**: Most unit tests use synthetic DataFrames generated in fixtures
- **Cached Real Data**: Integration tests may use cached Parquet files for realism
- **Historical Data**: Tests referencing real corporate actions (AAPL split, Enron delisting) should document data source
- **Versioning**: If cached test data changes, version files (e.g., `test_data_v1.parquet`)

### Test Refactoring Triggers

- **Heuristic Tuning**: If adjustment consistency thresholds change, update tests 1.4-UNIT-012, 013
- **New Validation Checks**: When adding new validation logic, add corresponding tests
- **Bridge API Changes**: If Norgate API changes, update bridge integration tests
- **Performance Degradation**: If performance slips, add new performance tests or update thresholds

---

## Integration with Quality Gates

### Gate YAML Block for Test Design

```yaml
test_design:
  date: 2025-12-05
  designer: Quinn
  scenarios_total: 20
  by_level:
    unit: 16
    integration: 4
    e2e: 0
  by_priority:
    p0: 9
    p1: 9
    p2: 2
  coverage_gaps: []
  risk_coverage:
    critical_risks_covered: 2/2
    high_risks_covered: 4/4
    medium_risks_covered: 4/6
  test_pyramid_compliance: true
  acceptance_criteria_coverage: 8/8
  recommended_gate: CONCERNS
  gate_rationale: |
    Test design is comprehensive with 100% coverage of critical and high risks.
    20 test scenarios provide strong validation of acceptance criteria.
    Integration tests depend on NDU availability - may need skip markers for CI.
    Gate remains CONCERNS due to underlying implementation risks (DATA-001, TECH-001)
    which tests can validate but not eliminate.
```

---

## Key Principles Applied

### 1. Shift Left
- 80% unit tests provide fast feedback on validation logic correctness
- Critical risk DATA-001 (validation correctness) addressed primarily through unit tests
- Integration tests focus on aspects that can't be unit tested (real bridge calls, full pipeline)

### 2. Risk-Based Testing
- P0 priority assigned to critical risks (DATA-001, TECH-001, TECH-002)
- P1 priority for high risks (DATA-002, PERF-001, DATA-003)
- Test design directly maps to risk profile findings

### 3. Efficient Coverage
- No duplicate coverage across test levels
- Each test has clear justification (risk coverage, AC validation)
- Integration tests validate aspects unit tests cannot (bridge, performance, full pipeline)

### 4. Maintainability
- One test per file follows Story 1.4 organization (deterministic test ID → file mapping)
- Fixtures in story-level conftest.py promote reuse
- Test IDs follow standard naming convention (1.4-{LEVEL}-{SEQ})

### 5. Fast Feedback
- P0 unit tests execute in < 1s
- Test execution order prioritizes fast-failing critical tests
- Performance test explicitly validates < 5s requirement

---

## Appendix: Test Data Requirements

### Synthetic Data Specifications

**Clean Dataset** (for baseline validation):
```
Tickers: AAPL, MSFT, GOOGL, AMZN, TSLA (5 tickers)
Date Range: 2020-01-01 to 2020-01-15 (10 trading days)
Schema: MultiIndex (date, symbol), OHLC columns, volume, dividend, unadjusted_close
Data Quality: No NaN, continuous dates, valid adjustments
```

**Corrupt Dataset** (for failure detection):
```
AAPL: 3 NaN values in 'close' column on random dates
MSFT: 2 NaN in 'open', 1 NaN in 'high'
TSLA: 10-day gap from 2020-06-01 to 2020-06-15
XYZ: 50% price jump without dividend (close: 100 → 150)
FAIL: Negative close price on one date (close: -50.0)
ENRN: Data ends on 2001-12-02 (delisted)
```

### Real Data Requirements

**Historical Corporate Actions**:
- Apple 7:1 stock split: June 9, 2014
- Enron delisting: December 2, 2001 (last trading day)

**Index Constituent Data**:
- Russell 1000 Current & Past as of 2010-01-01 (expect ~1000 tickers)

**Market Holidays** (for false positive prevention):
- Independence Day 2020: Friday, July 3 (market closed)
- Weekend dates: All Saturdays and Sundays

---

## Document Metadata

**Version**: 1.0
**Status**: Draft (Pending Implementation)
**Next Steps**:
1. Review test design with Development team
2. Implement test fixtures in `tests/stories/1.4/conftest.py`
3. Begin Sprint 1: Foundation tests (P0 critical path)
4. Update QA gate after implementation with test results

**References**:
- Risk Profile: `/home/frank/momo/docs/qa/assessments/1.4-risk-20251205.md`
- Story File: `/home/frank/momo/docs/stories/1.4.story.md`
- Test Levels Framework: `.bmad-core/data/test-levels-framework.md`
- Test Priorities Matrix: `.bmad-core/data/test-priorities-matrix.md`

---

**End of Test Design Document**
