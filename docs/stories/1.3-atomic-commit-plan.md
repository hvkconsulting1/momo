# Atomic Commit Plan: Story 1.3

**Story:** 1.3 - Implement Data Loading and Parquet Caching
**Total Commits:** 8
**Test Coverage:** 29/29 tests (16 P0, 10 P1, 3 P2)
**AC Coverage:** 8/8 acceptance criteria

---

## Commit 1: feat(cache): implement cache path generation and schema validation

**Tests:** 1.3-UNIT-005, 1.3-UNIT-006, 1.3-UNIT-007, 1.3-UNIT-008, 1.3-UNIT-009
**ACs:** AC3 (Cache to Parquet with organized naming)
**Priority:** P0

**Setup:**
- Create `tests/stories/1.3/conftest.py` with shared fixtures:
  - `sample_price_df`: Standard valid DataFrame for cache tests
  - `invalid_schema_dfs`: Collection of schema-violating DataFrames
  - `temp_cache_dir`: Temporary cache directory using pytest `tmp_path` fixture
- Create `tests/stories/1.3/unit/test_1_3_unit_005.py` - Test cache path generation consistency
- Create `tests/stories/1.3/unit/test_1_3_unit_006.py` - Test schema validation for required columns
- Create `tests/stories/1.3/unit/test_1_3_unit_007.py` - Test schema validation for correct dtypes
- Create `tests/stories/1.3/unit/test_1_3_unit_008.py` - Test MultiIndex structure validation
- Create `tests/stories/1.3/unit/test_1_3_unit_009.py` - Test rejection of empty DataFrames
- Create `src/momo/data/cache.py` with:
  - Add `CacheError` exception to `src/momo/utils/exceptions.py`
  - `get_cache_path(universe: str, start_date: date, end_date: date) -> Path` - Generates path using `{universe}_{start_date}_{end_date}.parquet` pattern
  - `_validate_price_schema(df: pd.DataFrame) -> None` - Validates schema (columns, dtypes, MultiIndex, non-empty)
  - Raise `CacheError` on validation failures with informative messages
  - All functions include type hints and comprehensive docstrings

**Verification:**
- Run: `uv run pytest tests/stories/1.3/unit/test_1_3_unit_00[5-9].py -v` (all 5 tests pass)
- Run: `uv run mypy src/momo/data/cache.py` (passes strict mode)
- Run: `uv run mypy src/momo/utils/exceptions.py` (passes strict mode)

---

## Commit 2: feat(cache): implement parquet save/load with round-trip validation

**Tests:** 1.3-UNIT-011, 1.3-UNIT-012, 1.3-UNIT-017, 1.3-INT-002, 1.3-INT-008, 1.3-INT-009
**ACs:** AC3, AC4, AC7
**Priority:** P0

**Setup:**
- Create `tests/stories/1.3/unit/test_1_3_unit_011.py` - Test `load_prices()` returns None when cache missing
- Create `tests/stories/1.3/unit/test_1_3_unit_012.py` - Test `load_prices()` returns DataFrame when cache exists
- Create `tests/stories/1.3/unit/test_1_3_unit_017.py` - Test round-trip equality (save → load → validate)
- Create `tests/stories/1.3/integration/test_1_3_int_002.py` - Test Parquet writes to correct directory path
- Create `tests/stories/1.3/integration/test_1_3_int_008.py` - Test loaded DataFrame has correct dtypes after I/O
- Create `tests/stories/1.3/integration/test_1_3_int_009.py` - Test MultiIndex preserved after Parquet I/O
- Implement in `src/momo/data/cache.py`:
  - `save_prices(df: pd.DataFrame, universe: str, start_date: date, end_date: date) -> Path` - Write DataFrame to Parquet
    - Call `_validate_price_schema(df)` before writing
    - Create cache directories if missing using `Path.mkdir(parents=True, exist_ok=True)`
    - Use pyarrow engine with snappy compression
    - Return path to saved file
  - `load_prices(universe: str, start_date: date, end_date: date) -> pd.DataFrame | None` - Read Parquet or return None
    - Return None if cache file doesn't exist
    - Load using pyarrow engine
    - Preserve MultiIndex structure

**Verification:**
- Run: `uv run pytest tests/stories/1.3/unit/test_1_3_unit_01[1-2,7].py -v` (3 unit tests pass)
- Run: `uv run pytest tests/stories/1.3/integration/test_1_3_int_00[2,8-9].py -v` (3 integration tests pass)
- Run: `uv run mypy src/momo/data/cache.py tests/stories/1.3/` (passes)

---

## Commit 3: feat(cache): add metadata tracking and invalidation

**Tests:** 1.3-UNIT-010, 1.3-UNIT-016, 1.3-INT-003
**ACs:** AC3, AC5
**Priority:** P1

**Setup:**
- Create `tests/stories/1.3/unit/test_1_3_unit_010.py` - Test metadata includes universe, date range, created_at, schema_version
- Create `tests/stories/1.3/unit/test_1_3_unit_016.py` - Test `invalidate()` removes cache files
- Create `tests/stories/1.3/integration/test_1_3_int_003.py` - Test Parquet uses snappy compression and pyarrow
- Enhance `src/momo/data/cache.py`:
  - Update `save_prices()` to include Parquet metadata:
    ```python
    metadata = {
        'universe': universe,
        'start_date': start_date.isoformat(),
        'end_date': end_date.isoformat(),
        'created_at': datetime.now(timezone.utc).isoformat(),
        'schema_version': '1.0'
    }
    df.to_parquet(path, engine='pyarrow', compression='snappy', metadata=metadata)
    ```
  - Implement `invalidate(universe: str, start_date: date, end_date: date) -> None` - Remove cache file
    - Use `Path.unlink(missing_ok=True)` to delete cache file
    - Log cache invalidation using structlog

**Verification:**
- Run: `uv run pytest tests/stories/1.3/unit/test_1_3_unit_01[0,6].py -v` (2 tests pass)
- Run: `uv run pytest tests/stories/1.3/integration/test_1_3_int_003.py -v` (1 test passes)
- Verify metadata is written: Use pyarrow to inspect Parquet metadata in integration test

---

## Commit 4: feat(loader): implement cache-first orchestration logic

**Tests:** 1.3-UNIT-001, 1.3-UNIT-013, 1.3-UNIT-014
**ACs:** AC1, AC4
**Priority:** P0

**Setup:**
- Create `tests/stories/1.3/unit/__init__.py` (if not exists)
- Create `tests/stories/1.3/integration/__init__.py` (if not exists)
- Create `tests/stories/1.3/unit/test_1_3_unit_001.py` - Test `load_universe()` fetches single symbol via bridge on cache miss
- Create `tests/stories/1.3/unit/test_1_3_unit_013.py` - Test `load_universe()` calls bridge only when cache misses
- Create `tests/stories/1.3/unit/test_1_3_unit_014.py` - Test `load_universe()` returns cached data on cache hit
- Create `src/momo/data/loader.py` with:
  - `load_universe(symbols: list[str], start_date: date, end_date: date, universe: str, force_refresh: bool = False) -> pd.DataFrame` - Orchestrate data loading
    - Import and use `cache.load_prices()` and `cache.save_prices()`
    - Import and use `bridge.fetch_price_data()`
    - Implement cache-first logic:
      1. If not force_refresh and cache exists, return cached data
      2. Otherwise, fetch from bridge and save to cache
    - Use sequential single-symbol fetching (batch optimization deferred)
    - Return MultiIndex DataFrame with (date, symbol) index
  - Type hints and comprehensive docstrings with DataFrame schema documentation

**Verification:**
- Run: `uv run pytest tests/stories/1.3/unit/test_1_3_unit_00[1,13-14].py -v` (3 tests pass)
- Run: `uv run mypy src/momo/data/loader.py` (passes)

---

## Commit 5: feat(loader): add force refresh and directory creation

**Tests:** 1.3-UNIT-002, 1.3-UNIT-015, 1.3-INT-004
**ACs:** AC5
**Priority:** P0

**Setup:**
- Create `tests/stories/1.3/unit/test_1_3_unit_002.py` - Test directory creation if missing
- Create `tests/stories/1.3/unit/test_1_3_unit_015.py` - Test `force_refresh=True` bypasses cache
- Create `tests/stories/1.3/integration/test_1_3_int_004.py` - Test force-refresh fetches and overwrites cache
- Enhance `src/momo/data/loader.py`:
  - Update `load_universe()` to handle `force_refresh=True`:
    - Skip cache loading when force_refresh is True
    - Fetch fresh data and overwrite existing cache
  - Ensure cache directories exist before writing (already handled by cache.save_prices from Commit 2)
  - Add logging for cache hit/miss/refresh events using structlog

**Verification:**
- Run: `uv run pytest tests/stories/1.3/unit/test_1_3_unit_00[2,15].py -v` (2 tests pass)
- Run: `uv run pytest tests/stories/1.3/integration/test_1_3_int_004.py -v` (1 test passes)
- Verify cache is overwritten when force_refresh=True in integration test

---

## Commit 6: feat(loader): add TOTALRETURN adjustment and progress logging

**Tests:** 1.3-UNIT-003, 1.3-UNIT-004
**ACs:** AC1, AC2
**Priority:** P0 (UNIT-004), P1 (UNIT-003)

**Setup:**
- Create `tests/stories/1.3/unit/test_1_3_unit_003.py` - Test progress logging for multi-symbol fetches
- Create `tests/stories/1.3/unit/test_1_3_unit_004.py` - Test TOTALRETURN adjustment passed to bridge
- Enhance `src/momo/data/loader.py`:
  - Update `load_universe()` to pass `adjustment="TOTALRETURN"` to `bridge.fetch_price_data()`
  - Add progress logging using structlog for multi-symbol fetches:
    - Log start: "fetching_universe", symbols_count=len(symbols)
    - Log per-symbol: "fetching_symbol", symbol=symbol, index=i, total=len(symbols)
    - Log completion: "universe_fetched", symbols_count=len(symbols), duration=elapsed

**Verification:**
- Run: `uv run pytest tests/stories/1.3/unit/test_1_3_unit_00[3-4].py -v` (2 tests pass)
- Verify TOTALRETURN is passed to bridge in unit test mocks
- Verify structlog outputs progress messages in unit test

---

## Commit 7: feat(loader): implement partial failure handling

**Tests:** 1.3-UNIT-018, 1.3-INT-001, 1.3-INT-010, 1.3-INT-011
**ACs:** AC1, AC2
**Priority:** P0 (UNIT-018, INT-001, INT-010), P1 (INT-011)

**Setup:**
- Create `tests/stories/1.3/unit/test_1_3_unit_018.py` - Test partial fetch failures handled gracefully
- Create `tests/stories/1.3/integration/test_1_3_int_001.py` - Test fetched data includes dividend column (TOTALRETURN)
- Create `tests/stories/1.3/integration/test_1_3_int_010.py` - Test cache consistency with partial failures
- Create `tests/stories/1.3/integration/test_1_3_int_011.py` - Test failed symbols logged with error details
- Enhance `src/momo/data/loader.py`:
  - Implement graceful partial failure handling in `load_universe()`:
    - Wrap each symbol fetch in try/except block
    - Collect errors: `failed_symbols: list[tuple[str, Exception]]`
    - Continue fetching remaining symbols on error
    - Save successfully fetched symbols to cache (partial results)
    - Log errors using structlog with failed symbol list
    - Raise warning or exception if ALL symbols fail
  - Handle bridge exceptions: `NDUNotRunningError`, `NorgateBridgeError`, `WindowsPythonNotFoundError`

**Verification:**
- Run: `uv run pytest tests/stories/1.3/unit/test_1_3_unit_018.py -v` (1 test passes)
- Run: `uv run pytest tests/stories/1.3/integration/test_1_3_int_00[1,10-11].py -v` (3 tests pass)
- Verify partial results are cached even when some symbols fail
- Verify dividend column present in integration test with real bridge call (requires NDU)

---

## Commit 8: test(perf): add performance validation benchmarks

**Tests:** 1.3-INT-005, 1.3-INT-006, 1.3-INT-007
**ACs:** AC6
**Priority:** P1 (INT-005, INT-007), P2 (INT-006)

**Setup:**
- Create `tests/stories/1.3/integration/test_1_3_int_005.py` - Test cache load time <50ms for 10 symbols
- Create `tests/stories/1.3/integration/test_1_3_int_006.py` - Test API fetch time ~500ms for 10 symbols
- Create `tests/stories/1.3/integration/test_1_3_int_007.py` - Test cache speedup >10x vs API
- Performance test implementation:
  - Use `time.perf_counter()` for precise timing measurements
  - Fetch 10 symbols over 1-month date range (2020-01-01 to 2020-01-31)
  - Test symbols: AAPL, MSFT, GOOGL, AMZN, TSLA, NVDA, META, NFLX, AMD, INTC
  - Compare cache load time vs fresh API fetch time
  - Assert cache <50ms, API ~500ms (allow ±50% tolerance), speedup >10x
  - Document baseline performance in test docstrings for future optimization

**Verification:**
- Run: `uv run pytest tests/stories/1.3/integration/test_1_3_int_00[5-7].py -v` (3 tests pass)
- Note: These tests require NDU running; may be marked as slow/optional
- Document performance baselines in test output for future stories

---

## Execution Order Summary

1. **Foundation - Cache Layer** (Commits 1-3): Build cache.py with path generation, schema validation, save/load, metadata
2. **Orchestration - Loader Layer** (Commits 4-6): Build loader.py with cache-first logic, force refresh, progress logging
3. **Robustness - Error Handling** (Commit 7): Add partial failure handling and integration validation
4. **Performance - Benchmarks** (Commit 8): Validate performance requirements and establish baselines

## Implementation Workflow for Each Commit

Each commit is a **logical unit** containing both tests and implementation:

1. **Implement the logical unit**:
   - Create test files with test scenarios for the commit
   - Implement the functionality to satisfy those tests
   - Order doesn't matter (TDD, test-after, or interleaved) - use what makes sense

2. **Verify**: Run the commit's tests to confirm they pass

3. **Commit**: Stage tests + implementation together with the commit message

**Key Principle:** One commit = one logical unit of functionality with its test coverage.

## Dependencies Between Commits

- **Commit 2** depends on Commit 1 (needs schema validation before save/load)
- **Commit 3** depends on Commit 2 (enhances save_prices with metadata)
- **Commit 4** depends on Commits 1-3 (uses cache.py functions)
- **Commit 5** depends on Commit 4 (enhances load_universe)
- **Commit 6** depends on Commit 5 (enhances load_universe)
- **Commit 7** depends on Commit 6 (enhances load_universe with error handling)
- **Commit 8** depends on Commits 1-7 (validates complete system)

**Linear Dependency Chain**: Must execute commits 1-8 in order.

---

## Test Execution Commands

### Run All Story 1.3 Tests
```bash
uv run pytest tests/stories/1.3/ -v
```

### Run by Priority
```bash
# P0 tests only (critical path - 16 tests)
uv run pytest tests/stories/1.3/ -m p0 -v

# P1 tests only (high priority - 10 tests)
uv run pytest tests/stories/1.3/ -m p1 -v

# P2 tests only (baseline/informational - 3 tests)
uv run pytest tests/stories/1.3/ -m p2 -v
```

### Run by Level
```bash
# Unit tests only (18 tests, <5 seconds)
uv run pytest tests/stories/1.3/unit/ -v

# Integration tests only (11 tests, <60 seconds, may require NDU)
uv run pytest tests/stories/1.3/integration/ -v
```

### Run with Coverage
```bash
uv run pytest tests/stories/1.3/ --cov=src/momo/data --cov-report=term-missing -v
```

### Progressive Test Execution (Recommended for Development)
```bash
# Phase 1: P0 Unit Tests (fail fast - 13 tests)
uv run pytest tests/stories/1.3/unit/ -m p0 -v --tb=short

# Phase 2: P0 Integration Tests (5 tests)
uv run pytest tests/stories/1.3/integration/ -m p0 -v --tb=short

# Phase 3: P1 Tests (both levels - 10 tests)
uv run pytest tests/stories/1.3/ -m p1 -v --tb=short

# Phase 4: P2 Tests (baseline - 3 tests)
uv run pytest tests/stories/1.3/ -m p2 -v --tb=short
```

---

## Notes for Implementation

### Schema Validation Requirements

From `docs/architecture/data-models.md`, the price data schema is:

```python
# Required columns and dtypes
PRICE_SCHEMA = {
    'date': 'datetime64[ns]',  # Index (MultiIndex with symbol)
    'symbol': 'str',            # Index (MultiIndex with date)
    'open': 'float64',
    'high': 'float64',
    'low': 'float64',
    'close': 'float64',
    'volume': 'int64',
    'unadjusted_close': 'float64',
    'dividend': 'float64'
}

# MultiIndex structure: (date, symbol)
```

### Cache Path Convention

```python
# Pattern: {universe}_{start_date}_{end_date}.parquet
# Example: russell_1000_cp_2010-01-01_2020-12-31.parquet
# Location: data/cache/prices/{filename}
```

### Exception Hierarchy

Add to `src/momo/utils/exceptions.py`:

```python
class DataError(Exception):
    """Base exception for data layer errors."""
    pass

class CacheError(DataError):
    """Cache operation errors."""
    pass
```

### Bridge Integration Pattern

From `src/momo/data/bridge.py`:

```python
# Use existing bridge function
from momo.data.bridge import fetch_price_data

# Fetch with TOTALRETURN adjustment
prices_df = fetch_price_data(
    symbol=symbol,
    start_date=start_date,
    end_date=end_date,
    adjustment="TOTALRETURN",
    timeout=30
)
```

### Logging Pattern

```python
import structlog

logger = structlog.get_logger()

# Log cache operations
logger.info("cache_hit", universe=universe, path=cache_path)
logger.info("fetching_universe", symbols_count=len(symbols))
logger.warning("partial_fetch_failure", failed_symbols=[...])
```

---

## Quality Checklist

Before finalizing each commit, verify:

- ✅ All tests for the commit pass
- ✅ Type checking passes (mypy strict mode)
- ✅ Code formatting passes (ruff format)
- ✅ All test functions have required markers (@pytest.mark.p0/p1/p2 AND @pytest.mark.unit/integration)
- ✅ Test docstrings include Test ID, Story, Priority, Test Level, Risk Coverage
- ✅ Implementation includes type hints and docstrings
- ✅ DataFrame schemas documented in docstrings
- ✅ structlog used for logging (not print or stdlib logging)
- ✅ No mutations of input DataFrames (use df.copy() if needed)
- ✅ Exception handling uses custom exceptions from momo.utils.exceptions

---

## Risk Mitigation Coverage

This commit plan addresses all high and medium risks from `1.3-risk-20251204.md`:

**High Risks (Score 6):**
- ✅ **DATA-001 (Parquet Schema Drift)**: Commits 1-2 implement comprehensive schema validation (UNIT-006, 007, 008, INT-008, 009)
- ✅ **TECH-002 (Bridge Integration Errors)**: Commit 7 implements partial failure handling (UNIT-018, INT-010, 011)

**Medium Risks (Score 4):**
- ✅ **TECH-001 (Cache Path Collision)**: Commit 1 ensures deterministic path generation (UNIT-005)
- ✅ **DATA-003 (Stale Cache Detection)**: Commits 3 and 5 implement metadata tracking and force refresh (UNIT-010, 015, 016, INT-004)
- ✅ **DATA-004 (Empty DataFrame Caching)**: Commit 1 validates non-empty DataFrames (UNIT-009)

**Low Risks (Score 2-3):**
- ✅ **DATA-002 (MultiIndex Preservation)**: Commit 2 validates MultiIndex round-trip (UNIT-008, INT-009)
- ✅ **PERF-001 (Sequential Fetching)**: Commit 8 establishes performance baseline (INT-005, 006, 007)
- ✅ **OPS-001 (Missing Cache Directory)**: Commit 5 ensures directory creation (UNIT-002)

---

## End of Plan

**Next Steps:** Use the `/atomic-commit` slash command to execute each commit individually, following the TDD workflow defined in the project's development process.
