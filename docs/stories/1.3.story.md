# Story 1.3: Implement Data Loading and Parquet Caching

## Status

**Draft**

## Story

**As a** developer,
**I want** to load historical price data from Norgate and cache it to local Parquet files,
**so that** backtests can run quickly and reproducibly without repeated API calls.

## Acceptance Criteria

1. `src/data/loader.py` module implements function to fetch OHLCV data for a list of tickers over a date range using norgatedata API
2. Fetched data includes adjustment factors for splits and dividends (adjusted prices)
3. Data is cached to Parquet files in `data/` directory with organized naming scheme (e.g., by universe or date range)
4. Cache loading function checks if local Parquet exists before querying Norgate API
5. Force-refresh option allows re-fetching data even if cache exists
6. Loading a cached dataset is >10x faster than API query (cache <50ms, API ~500ms for 10 symbols)
7. Cached Parquet files can be read back into pandas DataFrames with correct dtypes and index
8. Unit tests verify caching logic (write → read → validate equality)

## Tasks / Subtasks

- [ ] Implement Parquet cache manager module (AC: 3, 4, 7)
  - [x] Create `src/momo/data/cache.py` with cache manager functions
  - [x] Implement `get_cache_path()` helper to generate consistent file paths
  - [x] Ensure cache directories exist (create if missing using `Path.mkdir(parents=True, exist_ok=True)`)
  - [x] Implement `save_prices()` function to write DataFrame to Parquet with metadata
  - [x] Implement `load_prices()` function to read Parquet and return DataFrame or None
  - [x] Implement `invalidate()` function to force cache refresh
  - [x] Add cache metadata tracking (timestamp, universe, date range)
  - [x] Add type hints to all cache manager functions
  - [x] Document DataFrame schemas in docstrings

- [ ] Implement high-level data loader orchestrator (AC: 1, 2, 4, 5, 6)
  - [x] Create `src/momo/data/loader.py` with orchestration functions
  - [x] Implement `load_universe()` function coordinating bridge, cache, and validation
  - [x] Add `force_refresh` parameter to bypass cache
  - [x] Implement single-symbol fetching using bridge.fetch_price_data() (batch optimization deferred to future story)
  - [x] Add progress logging for multi-symbol fetches using structlog
  - [x] Ensure TOTALRETURN adjustment type is used for dividend-adjusted prices
  - [x] Add error handling for partial fetch failures (continue with remaining symbols)
  - [x] Add type hints and comprehensive docstrings

- [ ] Write unit tests for cache manager (AC: 8)
  - [x] Create test directory `tests/stories/1.3/unit/`
  - [x] Write unit test for `save_prices()` with sample DataFrame
  - [x] Write unit test for `load_prices()` returns None when cache missing
  - [x] Write unit test for `load_prices()` returns DataFrame when cache exists
  - [x] Write unit test for round-trip (save → load → validate equality)
  - [x] Write unit test for cache path generation consistency
  - [x] Write unit test for `invalidate()` function

- [ ] Write unit tests for data loader (AC: 8)
  - [x] Write unit test for `load_universe()` with cache hit (mock cache.load_prices)
  - [x] Write unit test for `load_universe()` with cache miss (mock bridge calls)
  - [x] Write unit test for `force_refresh=True` bypasses cache
  - [x] Write unit test for multi-symbol fetching logic (sequential single-symbol calls)
  - [x] Write unit test for error handling with partial fetch failures

- [ ] Write integration tests (AC: 6)
  - [x] Create test directory `tests/stories/1.3/integration/`
  - [x] Write integration test for full cache workflow (fetch → save → load → validate)
  - [ ] Write integration test for performance comparison (cache vs API fetch)
  - [ ] Write integration test for multi-symbol fetching (sequential single-symbol calls)
  - [x] Write integration test verifying DataFrame dtypes and index after cache round-trip
  - [x] Write integration test for force-refresh fetches and overwrites cache

## Dev Notes

### Previous Story Insights

Story 1.2 (Integrate Norgate Data API via Windows Python Bridge) completed successfully. Key findings relevant to this story:

- **Bridge Performance**: ~57ms per symbol (50ms subprocess overhead + 7ms NDU lookup)
- **Optimization Opportunity**: Current implementation fetches one symbol at a time. Story 1.0 showed batch fetching can reduce overhead to ~7ms per symbol (82 seconds for full Russell 3000 C&P with 12,225 symbols)
- **Bridge Functions Available**: `fetch_price_data(symbol, start_date, end_date, adjustment)` and `check_ndu_status()` from `src/momo/data/bridge.py`
- **Adjustment Types**: Use TOTALRETURN for backtesting (includes dividends), CAPITAL for technical analysis
- **Error Handling**: Bridge raises `NDUNotRunningError`, `WindowsPythonNotFoundError`, `NorgateBridgeError` for various failure modes

[Source: docs/stories/1.2.story.md]

### Tech Stack (Required Versions)

| Technology | Version | Purpose |
|------------|---------|---------|
| Python | 3.13 | Primary development language |
| pandas | 2.2.x | DataFrame operations, time series |
| pyarrow | 18.x | Parquet read/write (required for pandas Parquet support) |
| structlog | 24.x | Structured logging |

[Source: docs/architecture/tech-stack.md]

### Architecture Context

#### Data Layer File Locations

From the project source tree, the data layer files are located at:
- `src/momo/data/bridge.py` - Windows Python bridge (completed in Story 1.2)
- `src/momo/data/norgate.py` - Norgate API wrappers (future story)
- `src/momo/data/cache.py` - Parquet cache manager (to be created in this story)
- `src/momo/data/loader.py` - High-level data loading orchestrator (to be created in this story)
- `src/momo/data/validation.py` - Data quality checks (future story)
- `src/momo/data/universe.py` - Point-in-time universe construction (future story)

[Source: docs/architecture/source-tree.md:36-43]

#### Cache Storage Structure

From the source tree specification:
```
data/
├── cache/
│   ├── prices/                   # Cached price Parquet files
│   ├── constituents/             # Cached constituent Parquet files (future)
│   └── universes/                # Cached universe snapshots (future)
└── results/
    └── experiments/              # Experiment JSON records (future)
```

**File Naming Convention**: `{universe}_{start_date}_{end_date}.parquet`
Example: `russell_1000_cp_2010-01-01_2020-12-31.parquet`

[Source: docs/architecture/source-tree.md:9-15]

#### Data Models

**Price Data Schema** (from Architecture Data Models):

| Attribute | Type | Description |
|-----------|------|-------------|
| `date` | `datetime64[ns]` | Trading date (DataFrame index) |
| `symbol` | `str` | Ticker symbol (e.g., "AAPL", "LEHMQ-201203") |
| `open` | `float64` | Opening price (adjusted) |
| `high` | `float64` | High price (adjusted) |
| `low` | `float64` | Low price (adjusted) |
| `close` | `float64` | Closing price (adjusted) |
| `volume` | `int64` | Trading volume |
| `unadjusted_close` | `float64` | Raw close for reference |
| `dividend` | `float64` | Dividend amount (for TOTALRETURN adjustment) |

**DataFrame Format**: MultiIndex `(date, symbol)` for price data storage in cache.

[Source: docs/architecture/data-models.md:6-22]

#### Cache Manager Component Specification

From the Components architecture document:

**Responsibility**: Cache Norgate data to local Parquet files for offline operation and fast iteration.

**Key Interfaces**:
```python
def save_prices(df: pd.DataFrame, universe: str) -> Path
def load_prices(universe: str) -> pd.DataFrame | None
def save_constituents(df: pd.DataFrame, index: str) -> Path  # Future story
def load_constituents(index: str) -> pd.DataFrame | None      # Future story
def invalidate(universe: str) -> None
def get_cache_info() -> dict[str, CacheMetadata]              # Optional for this story
```

**Dependencies**: None (uses filesystem)

[Source: docs/architecture/components.md:33-48]

#### Data Loading Orchestrator Component Specification

From the Components architecture document:

**Responsibility**: Coordinate fetching data from Norgate (via bridge) or cache, with validation.

**Key Interfaces**:
```python
def load_universe(universe: str, start_date: date, end_date: date,
                  force_refresh: bool = False) -> pd.DataFrame
def load_constituents(index: str, start_date: date, end_date: date,
                      force_refresh: bool = False) -> pd.DataFrame  # Future story
def refresh_cache(universe: str, start_date: date, end_date: date) -> None  # Optional for this story
```

**Dependencies**: `bridge.py` (from Story 1.2), `cache.py` (this story), `validation.py` (future story - skip for now)

[Source: docs/architecture/components.md:49-63]

### Coding Standards

**Critical Rules** to follow:

1. **Always Use structlog** - Never `print()` or stdlib `logging`
2. **DataFrame Schema Documentation** - Document columns and dtypes in docstrings
3. **Explicit Date Handling** - Use `datetime.date`, not strings
4. **No Magic Numbers** - Name all constants or use config
5. **Validate at Layer Boundaries** - Data layer validates; inner layers trust input
6. **No Relative Imports Across Layers** - Use absolute imports from `momo`

**Naming Conventions**:
- DataFrame variables: Suffix `_df` (e.g., `prices_df`)
- Series variables: Suffix `_s` (e.g., `returns_s`)
- Date variables: Suffix `_date` (e.g., `start_date`)

[Source: docs/architecture/coding-standards.md:24-36]

### Exception Handling

Use custom exceptions from `src/momo/utils/exceptions.py` (created in Story 1.2):

```python
from momo.utils.exceptions import BridgeError, NorgateBridgeError

# Add new exception for this story:
class CacheError(DataError):
    """Cache operation errors."""
    pass
```

[Source: Inferred from docs/architecture/coding-standards.md:52 and Story 1.2 implementation]

### Performance Optimization Notes

From Story 1.0 findings:
- **Current**: ~57ms per symbol (subprocess overhead dominates)
- **Optimization Opportunity**: Batch fetch multiple symbols in one subprocess call
- **Target**: Reduce to ~7ms per symbol for batch operations
- **Full Russell 3000 C&P**: 12,225 symbols × 7ms = ~82 seconds (vs ~12 minutes with current implementation)

**Batch Fetching Strategy (DEFERRED TO FUTURE STORY)**:
```python
# Instead of calling fetch_price_data() 100 times (5.7 seconds)
# Construct one subprocess call that fetches all 100 symbols (0.7 seconds)
# Serialize results via JSON and parse in WSL Python
```

**Story 1.3 Scope**: This story implements single-symbol fetching using the existing `bridge.fetch_price_data()` function. Batch fetching optimization will be implemented in a future performance optimization story.

[Source: docs/research/norgate-api-exploration.md]

### Parquet Best Practices

**Engine**: Use `pyarrow` engine (already in dependencies)
**Compression**: Use `snappy` compression (good balance of speed and size)
**Index**: Preserve MultiIndex `(date, symbol)` when saving/loading

**Metadata Storage**: Store cache metadata using Parquet custom metadata via pandas `metadata` parameter:

```python
# Save with metadata
metadata = {
    'universe': 'russell_1000_cp',
    'start_date': '2010-01-01',
    'end_date': '2020-12-31',
    'created_at': datetime.now(timezone.utc).isoformat(),
    'momo_version': '0.1.0'
}
df.to_parquet(path, engine='pyarrow', compression='snappy', metadata=metadata)

# Load
df = pd.read_parquet(path, engine='pyarrow')

# Access metadata (if needed)
import pyarrow.parquet as pq
parquet_file = pq.ParquetFile(path)
stored_metadata = parquet_file.schema_arrow.metadata
```

[Source: Standard pandas/pyarrow practices for financial time series]

### Project Structure Notes

**Alignment**: Story requirements align with architecture documents. The cache and loader modules fit into the data layer as defined in source-tree.md.

**Key Points**:
- Cache manager is a foundational component for all future stories requiring data
- Loader orchestrates bridge, cache, and will coordinate with validation (Story 1.4)
- Future stories (1.4, 1.5) will extend the cache manager for constituents and universes

[Source: docs/architecture/source-tree.md]

## Testing

**Test File Location**:
- Story-based location: `tests/stories/1.3/`
- Unit tests: `tests/stories/1.3/unit/`
- Integration tests: `tests/stories/1.3/integration/`

**Test Standards**:
- File naming: `test_1_3_unit_001.py`, `test_1_3_int_001.py`, etc.
- Test function naming: `test_1_3_unit_001()`, `test_1_3_int_001()`, etc.
- One test per file principle (deterministic AI agent mapping)
- Include docstring header with Test ID, Story, Priority, Test Level, Risk Coverage

**Testing Frameworks and Patterns**:
- Framework: pytest 8.x
- Mocking: unittest.mock for cache file I/O and bridge calls
- Integration tests require actual file system operations (but not NDU)
- Use `tmp_path` fixture for temporary cache directories in tests

**Test Execution**:
```bash
# Run all Story 1.3 tests
pytest tests/stories/1.3/ -v

# Run only unit tests (no file I/O)
pytest tests/stories/1.3/unit/ -v

# Run only integration tests (actual file I/O)
pytest tests/stories/1.3/integration/ -v

# Run with coverage
pytest tests/stories/1.3/ --cov=src/momo/data --cov-report=term-missing
```

**Specific Test Requirements**:

**Unit Tests** (tests/stories/1.3/unit/):
1. **Cache Manager Tests**:
   - Verify `save_prices()` writes Parquet with correct schema (mock file I/O)
   - Verify `load_prices()` returns None when cache missing (mock file system)
   - Verify `load_prices()` returns DataFrame when cache exists (mock file I/O)
   - Verify round-trip equality (save → load → compare DataFrames)
   - Verify cache path generation follows naming convention
   - Verify `invalidate()` function removes cache files

2. **Data Loader Tests**:
   - Verify `load_universe()` with cache hit (mock cache.load_prices returns DataFrame)
   - Verify `load_universe()` with cache miss (mock cache returns None, then mock bridge calls)
   - Verify `force_refresh=True` bypasses cache and calls bridge
   - Verify multi-symbol fetching using sequential single-symbol bridge calls
   - Verify error handling with partial fetch failures

**Integration Tests** (tests/stories/1.3/integration/):
1. **Full Cache Workflow**: Fetch data via bridge → save to cache → load from cache → validate DataFrame equality
2. **Performance Comparison**: Measure cache load time vs bridge fetch time (cache <50ms, API ~500ms for 10 symbols, >10x speedup)
3. **Multi-Symbol Fetching**: Fetch 10+ symbols sequentially, verify all present in cached DataFrame
4. **DataFrame Schema Validation**: Verify dtypes and index structure after cache round-trip

[Source: docs/architecture/test-strategy-and-standards.md:1-630]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-04 | 1.0 | Story created from Epic 1 | Bob (Scrum Master) |
| 2025-12-04 | 1.1 | Story refinements post-review: deferred batch fetching to future story, added Parquet metadata specification, quantified AC6 performance threshold, added cache directory creation subtask | Frank (via Claude Code) |

## Dev Agent Record

### Agent Model Used

claude-sonnet-4-5-20250929

### Debug Log References

- [Commit 1] No issues encountered - schema validation implementation straightforward

### Completion Notes

- [Commit 1] Successfully implemented cache path generation and comprehensive schema validation for Parquet caching foundation
- [Commit 2] Implemented save_prices() and load_prices() functions with automatic directory creation, achieving full Parquet round-trip validation
- [Commit 3] Added Parquet metadata tracking (universe, date range, created_at, schema_version) and cache invalidation with structlog logging
- [Commit 4] Implemented cache-first orchestration in load_universe() function with sequential single-symbol fetching and TOTALRETURN adjustment
- [Commit 5] Enhanced load_universe() with force_refresh parameter and comprehensive cache refresh logging (cache_hit, cache_miss, cache_refresh events)
- [Commit 6] Added comprehensive progress logging (fetching_universe, fetching_symbol with index/total, universe_fetched with duration) and verified TOTALRETURN adjustment propagation to bridge
- [Commit 7] Implemented graceful partial failure handling with try/except blocks around bridge calls, error logging (symbol_fetch_failed), partial failure warnings, and continued fetching of remaining symbols after errors. Updated bridge.py to include dividend and unadjusted_close columns in fetched data, and created MultiIndex (date, symbol) after concatenating symbol DataFrames

### File List

- src/momo/data/cache.py - Parquet cache manager with path generation, schema validation, metadata tracking, and invalidation
- src/momo/data/loader.py - High-level data loader orchestrator with cache-first logic and force_refresh parameter
- src/momo/utils/exceptions.py - Added DataError and CacheError exception classes
- mypy.ini - Updated to exclude test __init__.py files from mypy checking
- pytest.ini - Updated python_files pattern to exclude __init__.py from test collection
- tests/stories/1.3/conftest.py - Shared fixtures for story 1.3 tests
- tests/stories/1.3/unit/test_1_3_unit_001.py - Test load_universe() fetches via bridge on cache miss
- tests/stories/1.3/unit/test_1_3_unit_002.py - Test directory creation if missing during cache write
- tests/stories/1.3/unit/test_1_3_unit_005.py - Test cache path generation consistency
- tests/stories/1.3/unit/test_1_3_unit_006.py - Test schema validation for required columns
- tests/stories/1.3/unit/test_1_3_unit_007.py - Test schema validation for correct dtypes
- tests/stories/1.3/unit/test_1_3_unit_008.py - Test MultiIndex structure validation
- tests/stories/1.3/unit/test_1_3_unit_009.py - Test rejection of empty DataFrames
- tests/stories/1.3/unit/test_1_3_unit_010.py - Test metadata tracking in Parquet files
- tests/stories/1.3/unit/test_1_3_unit_011.py - Test load_prices() returns None on cache miss
- tests/stories/1.3/unit/test_1_3_unit_012.py - Test load_prices() returns DataFrame on cache hit
- tests/stories/1.3/unit/test_1_3_unit_013.py - Test load_universe() calls bridge only on cache miss
- tests/stories/1.3/unit/test_1_3_unit_014.py - Test load_universe() returns cached data on cache hit
- tests/stories/1.3/unit/test_1_3_unit_015.py - Test force_refresh=True bypasses cache
- tests/stories/1.3/unit/test_1_3_unit_016.py - Test cache invalidation removes files
- tests/stories/1.3/unit/test_1_3_unit_017.py - Test round-trip equality (save → load → validate)
- tests/stories/1.3/integration/test_1_3_int_002.py - Test Parquet writes to correct directory path
- tests/stories/1.3/integration/test_1_3_int_003.py - Test Parquet uses snappy compression and pyarrow engine
- tests/stories/1.3/integration/test_1_3_int_004.py - Test force-refresh fetches and overwrites cache
- tests/stories/1.3/integration/test_1_3_int_008.py - Test loaded DataFrame has correct dtypes after I/O
- tests/stories/1.3/integration/test_1_3_int_009.py - Test MultiIndex preserved after Parquet I/O
- tests/stories/1.3/unit/test_1_3_unit_003.py - Test progress logging for multi-symbol fetches (fetching_universe, fetching_symbol, universe_fetched)
- tests/stories/1.3/unit/test_1_3_unit_004.py - Test TOTALRETURN adjustment passed to bridge calls
- tests/stories/1.3/unit/test_1_3_unit_018.py - Test partial fetch failures handled gracefully (continue with remaining symbols, log errors, save partial results)
- tests/stories/1.3/integration/test_1_3_int_001.py - Test TOTALRETURN adjustment includes dividend column
- tests/stories/1.3/integration/test_1_3_int_010.py - Test cache consistency with partial fetch failures
- tests/stories/1.3/integration/test_1_3_int_011.py - Test failed symbols logged with error details
- src/momo/data/bridge.py - Updated to include dividend and unadjusted_close columns in schema
- tests/stories/1.2/unit/test_1_2_unit_008.py - Updated mock data to include dividend and unadjusted_close
- tests/stories/1.2/unit/test_1_2_unit_009.py - Updated mock data and assertions to include new columns

## QA Results

(To be filled by QA Agent)
